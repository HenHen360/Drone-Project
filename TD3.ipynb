{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "Connected to AirSim!\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Main libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import airsim as airsim\n",
    "from airsim import MultirotorClient\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#General\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from typing import Dict, Any, Tuple\n",
    "import logging\n",
    "from logging import handlers\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn.utils as torch_utils\n",
    "import asyncio\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "client = airsim.MultirotorClient()\n",
    "client.confirmConnection()\n",
    "print(\"Connected to AirSim!\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDroneCosysAirSimEnv():\n",
    "    def __init__(self, num_drones=5, training_stage=0):\n",
    "        self.client = airsim.MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        \n",
    "        self.num_drones = num_drones\n",
    "        self.drone_names = [f'Drone{i}' for i in range(num_drones)]\n",
    "        \n",
    "        # Action and observation spaces remain the same for each drone\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(21,), dtype=np.float32)\n",
    "        \n",
    "        self.episode = 0\n",
    "        self.wind_factor = 0.0\n",
    "        self.success_count = 0\n",
    "        self.success_threshold = 10\n",
    "        self.stage_threshold = 25\n",
    "        \n",
    "        \n",
    "        # Track state for each drone\n",
    "        self.drone_states = {\n",
    "            drone_name: {\n",
    "                'current_step': 0,\n",
    "                'success_count': 0,\n",
    "                'consecutive_success': 0,\n",
    "                'current_target': None,\n",
    "                'prev_distance': None,\n",
    "                'reseting': False,\n",
    "                'resetTick': 0,\n",
    "                'resetingTime': 0,\n",
    "                'currentTotalReward': 0,\n",
    "            } for drone_name in self.drone_names\n",
    "        }\n",
    "        self.training_stage = training_stage\n",
    "        self.setup_stage_params()\n",
    "        self.initialize_drones()\n",
    "        \n",
    "    def initialize_drones(self):\n",
    "        \"\"\"Initialize all drones with proper controls and initial positions\"\"\"\n",
    "        for drone_name in self.drone_names:\n",
    "            self.client.enableApiControl(True, drone_name)\n",
    "            self.client.armDisarm(True, drone_name)\n",
    "            \n",
    "        # Generate initial target\n",
    "        self.current_target = self._generate_new_target()\n",
    "        for drone_name in self.drone_names:\n",
    "            self.drone_states[drone_name]['current_target'] = self.current_target\n",
    "\n",
    "    def setup_stage_params(self):\n",
    "        self.params = {\n",
    "            0: {  # Hovering stage\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 5.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'hover',\n",
    "                'height_range': [-2.0, -1.5],  # Desired hover height\n",
    "                'target_radius': 0.5\n",
    "            },\n",
    "            1: {  # Near vertical movement\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 8.0],\n",
    "                'max_steps': 250,\n",
    "                'target_type': 'vertical',\n",
    "                'height_range': [-4.0, -1.0],\n",
    "                'target_radius': 0.3\n",
    "            },\n",
    "            2: {  # Close-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 5.0\n",
    "            },\n",
    "            3: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 400,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 10.0\n",
    "            },\n",
    "            4: {  # Long-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0\n",
    "            },\n",
    "            5: {  # Extended range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [30.0, 30.0, 15.0],  # Expanded area\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 25.0  # Increased maximum distance\n",
    "            },\n",
    "            6: {  # Recovery training\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 50  # Apply random action every 50 steps\n",
    "            },\n",
    "            7: {  # Moderate wind\n",
    "                'wind': 0.5,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,\n",
    "                'max_target_dist': 25.0\n",
    "            },\n",
    "            8: {  # Strong wind\n",
    "                'wind': 1.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,  # Slightly larger radius due to strong wind\n",
    "                'max_target_dist': 25.0\n",
    "            }\n",
    "        }\n",
    "        self.current_params = self.params[self.training_stage]\n",
    "\n",
    "    def _generate_new_target(self):\n",
    "        \"\"\"Internal method to generate a new target based on current parameters\"\"\"\n",
    "        params = self.current_params\n",
    "        if params['target_type'] == 'hover':\n",
    "            return np.array([0.0, 0.0, -1.75])\n",
    "            \n",
    "        elif params['target_type'] == 'vertical':\n",
    "            height = np.random.uniform(*params['height_range'])\n",
    "            return np.array([0.0, 0.0, height])\n",
    "            \n",
    "        elif params['target_type'] == 'horizontal_near':\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            dist = np.random.uniform(2.0, params['max_target_dist'])\n",
    "            return np.array([\n",
    "                dist * np.cos(angle),\n",
    "                dist * np.sin(angle),\n",
    "                -2.0\n",
    "            ])\n",
    "            \n",
    "        elif params['target_type'] in ['free_near', 'free']:\n",
    "            while True:\n",
    "                point = np.random.uniform(\n",
    "                    low=[-params['max_target_dist'], -params['max_target_dist'], -4.0],\n",
    "                    high=[params['max_target_dist'], params['max_target_dist'], -1.0],\n",
    "                    size=(3,)\n",
    "                )\n",
    "                if np.linalg.norm(point - self._get_current_position()) > 2.0:\n",
    "                    return point\n",
    "\n",
    "    def _get_target(self):\n",
    "        \"\"\"Return the current target\"\"\"\n",
    "        return self.current_target\n",
    "\n",
    "    def _get_current_position(self, drone_name: str):\n",
    "        state = self.client.getMultirotorState(vehicle_name=drone_name)\n",
    "        pos = state.kinematics_estimated.position\n",
    "        return np.array([pos.x_val, pos.y_val, pos.z_val], dtype=np.float32)\n",
    "\n",
    "    def _get_current_velocity(self, drone_name: str):\n",
    "        state = self.client.getMultirotorState(vehicle_name=drone_name)\n",
    "        vel = state.kinematics_estimated.linear_velocity\n",
    "        return np.array([vel.x_val, vel.y_val, vel.z_val], dtype=np.float32)\n",
    "    \n",
    "    def _get_imu_data(self, drone_name: str):\n",
    "        imu_data = self.client.getImuData(vehicle_name=drone_name)\n",
    "        ang_vel = imu_data.angular_velocity\n",
    "        lin_acc = imu_data.linear_acceleration\n",
    "        return np.array([\n",
    "            ang_vel.x_val, ang_vel.y_val, ang_vel.z_val,\n",
    "            lin_acc.x_val, lin_acc.y_val, lin_acc.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self, drone_name: str):\n",
    "        position = self._get_current_position(drone_name)\n",
    "        velocity = self._get_current_velocity(drone_name)\n",
    "        imu_data = self._get_imu_data(drone_name)\n",
    "        target = self.drone_states[drone_name]['current_target']\n",
    "        distance = np.linalg.norm(position - target)\n",
    "        \n",
    "        area_bounds = np.array(self.current_params['area'])\n",
    "        normalized_pos = np.clip(position / area_bounds, -1.0, 1.0)\n",
    "        velocity_scale = np.array([10.0, 10.0, 5.0])\n",
    "        normalized_vel = np.clip(velocity / velocity_scale, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize IMU data\n",
    "        angular_vel = np.clip(imu_data[:3] / 5.0, -1.0, 1.0)\n",
    "        linear_acc = np.clip(imu_data[3:6] / 9.81, -3.0, 3.0)  # Allow for higher G-forces\n",
    "        \n",
    "        # Normalize target position\n",
    "        normalized_target = np.clip(target / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize distance relative to maximum possible distance in current area\n",
    "        max_possible_distance = np.linalg.norm(area_bounds)\n",
    "        normalized_distance = np.clip(distance / max_possible_distance, 0.0, 1.0)\n",
    "        \n",
    "        # Height error normalization\n",
    "        height_error = (position[2] - target[2]) / area_bounds[2]\n",
    "        normalized_height_error = np.clip(height_error, -1.0, 1.0)\n",
    "\n",
    "        relative_pos = (target - position) / area_bounds\n",
    "        target_direction = (target - position) / (np.linalg.norm(target - position) + 1e-6)\n",
    "        relative_vel = np.dot(velocity, target_direction)\n",
    "        \n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]\n",
    "            normalized_vel,        # [3:6]\n",
    "            angular_vel,          # [6:9]\n",
    "            linear_acc,          # [9:12]\n",
    "            normalized_target,  # [12:15]\n",
    "            relative_pos,        # [15:18]\n",
    "            [normalized_distance],# [18]\n",
    "            [normalized_height_error],      # [19]\n",
    "            [relative_vel]       # [20]\n",
    "        ])\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def reset_step(self, drone_name):\n",
    "        #Handle the stepwise reset process for a single drone\n",
    "        drone_state = self.drone_states[drone_name]\n",
    "        \n",
    "        if drone_state['reseting'] == True:\n",
    "            current_time = time.perf_counter()\n",
    "            \n",
    "            try:\n",
    "                # Initial reset\n",
    "                if drone_state['resetTick'] == 0 and drone_state['resetingTime'] == 0:\n",
    "                    drone_state['resetingTime'] = current_time\n",
    "                    self.client.client.call_async(\n",
    "                        \"resetVehicle\",\n",
    "                        drone_name,\n",
    "                        airsim.Pose(\n",
    "                            airsim.Vector3r(0, 0, 0),  # Reset to origin\n",
    "                            airsim.Quaternionr(0.0, 0.0, 0.0, 1.0)\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    drone_state['resetTick'] = 1\n",
    "                    \n",
    "                elif drone_state['resetTick'] == 2 and current_time - drone_state['resetingTime'] > 3.0:\n",
    "                    # Then arm\n",
    "                    self.client.armDisarm(True, drone_name)\n",
    "                    time.sleep(0.1)  # Wait for arming to complete\n",
    "                    drone_state['resetTick'] = 3\n",
    "                    \n",
    "                # Initiate takeoff - State 3\n",
    "                elif drone_state['resetTick'] == 3 and current_time - drone_state['resetingTime'] > 4.0:\n",
    "                    # Use hover instead of takeoff for more stability\n",
    "                    self.client.hoverAsync(vehicle_name=drone_name)\n",
    "                    drone_state['resetTick'] = 4\n",
    "                    \n",
    "                elif drone_state['resetTick'] == 4 and time.perf_counter() - drone_state['resetingTime'] > 6.0:\n",
    "                    drone_state['reseting'] = False\n",
    "                    drone_state['resetTick'] = 0\n",
    "                    drone_state['current_step'] = 0\n",
    "                    drone_state['currentTotalReward'] = 0\n",
    "                    drone_state['resetingTime'] = 0\n",
    "                    drone_state['consecutive_success'] = 0\n",
    "                    self.episode += 1\n",
    "                    print(self.episode)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during reset for {drone_name}: {str(e)}\")\n",
    "                # If there's an error, try to safely disable everything\n",
    "                try:\n",
    "                    self.client.armDisarm(False, drone_name)\n",
    "                    self.client.enableApiControl(False, drone_name)\n",
    "                except:\n",
    "                    pass\n",
    "                # Reset the reset process\n",
    "                drone_state['resetTick'] = 0\n",
    "                drone_state['resetingTime'] = 0\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        #Force reset all drones regardless of state\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        \n",
    "        observations = {}\n",
    "        infos = {}\n",
    "        self.client.reset()\n",
    "        # Reset all drones\n",
    "        for drone_name in self.drone_names:\n",
    "            self.client.enableApiControl(True, drone_name)\n",
    "            self.client.armDisarm(True, drone_name)\n",
    "            self.client.takeoffAsync(drone_name)\n",
    "            # Reset state\n",
    "            self.drone_states[drone_name].update({\n",
    "                'current_step': 0,\n",
    "                'success_count': 0,\n",
    "                'consecutive_success': 0,\n",
    "                'current_target': None,\n",
    "                'prev_distance': None,\n",
    "                'reseting': False,\n",
    "                'resetTick': 0,\n",
    "                'resetingTime': 0,\n",
    "                'currentTotalReward': 0\n",
    "            })\n",
    "            self.current_target = self._generate_new_target()\n",
    "            self.drone_states[drone_name]['current_target'] = self.current_target\n",
    "            \n",
    "            observations[drone_name] = self._get_observation(drone_name)\n",
    "            infos[drone_name] = {}\n",
    "            \n",
    "        return observations, infos\n",
    "    \n",
    "    def step(self, actions: Dict[str, np.ndarray]):\n",
    "            # actions should be a dict mapping drone_name to action array\n",
    "            observations = {}\n",
    "            rewards = {}\n",
    "            terminateds = {}\n",
    "            truncateds = {}\n",
    "            infos = {}\n",
    "\n",
    "            # Filter out resetting drones\n",
    "            active_drones = {name: action for name, action in actions.items() \n",
    "                            if not self.drone_states[name]['reseting']}\n",
    "            \n",
    "            if active_drones:\n",
    "                # Handle each drone's movement individually since there's no batch roll/pitch command\n",
    "                for drone_name, action in active_drones.items():\n",
    "                    self.drone_states[drone_name]['current_step'] += 1\n",
    "                    action = np.clip(action, -1.0, 1.0)\n",
    "                    \n",
    "                    # Process controls for each drone\n",
    "                    max_vx, max_vy, max_vz = 3.0, 3.0, 3.0\n",
    "                    max_yaw_deg = 45.0\n",
    "                    vx = float(action[0]) * max_vx\n",
    "                    vy = float(action[1]) * max_vy\n",
    "                    vz = float(action[2]) * max_vz\n",
    "                    yaw_rate_deg = float(action[3]) * max_yaw_deg\n",
    "                    \n",
    "                    yaw_mode = airsim.YawMode(is_rate=True, yaw_or_rate=yaw_rate_deg)\n",
    "\n",
    "                    # Execute movement for each drone\n",
    "                    self.client.moveByVelocityBodyFrameAsync(\n",
    "                        vx=vx,\n",
    "                        vy=vy,\n",
    "                        vz=vz,\n",
    "                        duration=0.2,  # small step\n",
    "                        drivetrain=airsim.DrivetrainType.MaxDegreeOfFreedom,\n",
    "                        yaw_mode=yaw_mode,\n",
    "                        vehicle_name = drone_name\n",
    "                    )\n",
    "\n",
    "            # Handle resetting drones\n",
    "            for drone_name in self.drone_names:\n",
    "                if self.drone_states[drone_name]['reseting']:\n",
    "                    self.reset_step(drone_name)\n",
    "\n",
    "            # Process observations and rewards for all drones\n",
    "            for drone_name in actions.keys():\n",
    "                if self.drone_states[drone_name]['reseting']:\n",
    "                    # For resetting drones, provide zero observations and rewards\n",
    "                    observations[drone_name] = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "                    rewards[drone_name] = 0.0\n",
    "                    terminateds[drone_name] = True\n",
    "                    truncateds[drone_name] = False\n",
    "                else:\n",
    "                    # Normal processing for active drones\n",
    "                    observations[drone_name] = self._get_observation(drone_name)\n",
    "                    reward, terminated = self._calculate_reward(drone_name)\n",
    "                    rewards[drone_name] = reward\n",
    "                    terminateds[drone_name] = terminated\n",
    "                    truncateds[drone_name] = self.drone_states[drone_name]['current_step'] >= self.current_params['max_steps']\n",
    "                    \n",
    "                    if terminated or truncateds[drone_name]:\n",
    "                        self.drone_states[drone_name]['reseting'] = True\n",
    "\n",
    "                infos[drone_name] = {\n",
    "                    'success_count': self.drone_states[drone_name]['success_count'],\n",
    "                    'stage': self.training_stage,\n",
    "                    'resetting': self.drone_states[drone_name]['reseting']\n",
    "                }\n",
    "\n",
    "            return observations, rewards, terminateds, truncateds, infos\n",
    "\n",
    "    def _calculate_reward(self, drone_name):\n",
    "        pos = self._get_current_position(drone_name)\n",
    "        vel = self._get_current_velocity(drone_name)\n",
    "        angular_vel = self._get_imu_data(drone_name)[:3]\n",
    "        target = self.drone_states[drone_name]['current_target']\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        reward_scale = 0.1\n",
    "        reward = 0.0\n",
    "\n",
    "        # Immediate failure conditions\n",
    "        if self.client.simGetCollisionInfo(vehicle_name=drone_name).has_collided:\n",
    "            reward -= 100 * reward_scale\n",
    "            return reward, True\n",
    "\n",
    "        if any(abs(p) > a for p, a in zip(pos, self.current_params['area'])):\n",
    "            reward -= 50 * reward_scale\n",
    "            return reward, True\n",
    "\n",
    "        # Main distance reward - smoother gradient\n",
    "        distance_reward = -distance + 1\n",
    "\n",
    "        if self.training_stage == 0:  # Hovering\n",
    "            # Pure hovering - focus on stability\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0) * reward_scale\n",
    "            reward += (distance_reward * 0.5) * reward_scale\n",
    "\n",
    "        elif self.training_stage == 1:  # Vertical movement\n",
    "            # Reward vertical progress toward target\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (2.0 * np.exp(-vertical_distance)) * reward_scale\n",
    "            \n",
    "            # Penalize horizontal drift more strongly\n",
    "            horizontal_drift = np.linalg.norm(pos[:2] - target[:2])\n",
    "            reward -= (0.5 * horizontal_drift) * reward_scale\n",
    "            \n",
    "            reward += (distance_reward * 0.5) * reward_scale\n",
    "\n",
    "        elif self.training_stage == 2:  # Close-range horizontal\n",
    "            # Reward horizontal progress\n",
    "            prev_distance = self.drone_states[drone_name]['prev_distance']\n",
    "            if prev_distance is not None:\n",
    "                progress = prev_distance - distance\n",
    "                reward += (3.0 * progress) * reward_scale  # Stronger reward for deliberate movement\n",
    "            self.drone_states[drone_name]['prev_distance'] = distance\n",
    "            \n",
    "            reward += distance_reward * reward_scale\n",
    "\n",
    "        elif self.training_stage >= 3:  # Medium-range movement\n",
    "            prev_distance = self.drone_states[drone_name]['prev_distance']\n",
    "            if prev_distance is not None:\n",
    "                progress = prev_distance - distance\n",
    "                reward += (2.0 * progress) * reward_scale\n",
    "            self.drone_states[drone_name]['prev_distance'] = distance\n",
    "            reward += distance_reward * reward_scale\n",
    "\n",
    "        # Success condition\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance < success_radius:\n",
    "            bonus = 0\n",
    "            bonus += 25.0 * reward_scale\n",
    "            self.drone_states[drone_name]['consecutive_success'] += 1\n",
    "            \n",
    "            if self.drone_states[drone_name]['consecutive_success'] % self.success_threshold == 0:\n",
    "                self.current_target = self._generate_new_target()\n",
    "                # Update target for all drones since they share the same target\n",
    "                for name in self.drone_names:\n",
    "                    self.drone_states[name]['current_target'] = self.current_target\n",
    "                bonus += 50.0 * reward_scale\n",
    "                self.drone_states[drone_name]['success_count'] += 1\n",
    "            \n",
    "            if self.drone_states[drone_name]['success_count'] >= self.stage_threshold:\n",
    "                self.training_stage = min(8, self.training_stage + 1)\n",
    "                print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "                # Reset success counts for all drones when advancing stage\n",
    "                for name in self.drone_names:\n",
    "                    self.drone_states[name]['success_count'] = 0\n",
    "                self.setup_stage_params()\n",
    "                \n",
    "            reward += bonus\n",
    "            return reward, True\n",
    "\n",
    "        return reward, False\n",
    "    def close(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "        self.client.armDisarm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "        # Use float32 for all arrays to match network precision\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(max_size, dtype=np.float32)  # Flattened array\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros(max_size, dtype=np.float32)    # Flattened array\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Ensure incoming data is on CPU and in numpy format\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(action):\n",
    "            action = action.detach().cpu().numpy()\n",
    "        if torch.is_tensor(reward):\n",
    "            reward = reward.detach().cpu().numpy()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(done):\n",
    "            done = done.detach().cpu().numpy()\n",
    "            \n",
    "        np.copyto(self.states[self.ptr], state)\n",
    "        np.copyto(self.actions[self.ptr], action)\n",
    "        self.rewards[self.ptr] = reward\n",
    "        np.copyto(self.next_states[self.ptr], next_state)\n",
    "        self.dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize buffers with correct shapes\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((max_size, 1), dtype=np.float32)  # Changed shape to (max_size, 1)\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size, 1), dtype=np.float32)    # Changed shape to (max_size, 1)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to numpy arrays and ensure correct shapes\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        reward = np.array(reward, dtype=np.float32).reshape(1)\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        done = np.array(done, dtype=np.float32).reshape(1)\n",
    "\n",
    "        # Store transition\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        # Update pointer and size\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature_net(state)\n",
    "        return self.max_action * self.action_net(features)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(act_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_features = self.state_net(state)\n",
    "        action_features = self.action_net(action)\n",
    "        features = torch.cat([state_features, action_features], dim=-1)\n",
    "        return self.q_net(features)\n",
    "    \"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        \n",
    "        # Use orthogonal initialization\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.q2_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for net in [self.q1_net, self.q2_net]:\n",
    "            for m in net.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        # Get Q-values\n",
    "        q1 = self.q1_net(sa)\n",
    "        q2 = self.q2_net(sa)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        return self.q1_net(sa)\n",
    "\"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.001):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.target_entropy = -float(actor.__init_args__[1])\n",
    "        \n",
    "        # Initialize episode tracking\n",
    "        self.current_episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        \n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it, noise):\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        reward = reward.view(-1)\n",
    "        done = done.view(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_Q1 = self.critic_target1(next_state, next_action)\n",
    "            target_Q2 = self.critic_target2(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            \n",
    "            if len(reward.shape) == 1:\n",
    "                reward = reward.unsqueeze(-1)\n",
    "            if len(done.shape) == 1:\n",
    "                done = done.unsqueeze(-1)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "        \n",
    "        # Critic 1 update\n",
    "        current_Q1 = self.critic1(state, action)\n",
    "        critic_mse_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_l2_reg1 = 0.00001 * sum(torch.sum(param ** 2) for param in self.critic1.parameters())\n",
    "        critic_loss1 = critic_mse_loss1 + critic_l2_reg1\n",
    "        \n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward(retain_graph=True) \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), 1.0)\n",
    "        self.critic_optimizer1.step()\n",
    "        \n",
    "        # Critic 2 update\n",
    "        current_Q2 = self.critic2(state, action)\n",
    "        critic_mse_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "        critic_l2_reg2 = 0 #0.00001 * sum(torch.sum(param ** 2) for param in self.critic2.parameters())\n",
    "        critic_loss2 = critic_mse_loss2 + critic_l2_reg2\n",
    "        \n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), 1.0)\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        if total_it % policy_delay == 0:\n",
    "            actor_actions = self.actor(state)\n",
    "            actor_loss = -self.critic1(state, actor_actions).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None\n",
    "        }\n",
    "    \"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self._hard_update_targets()\n",
    "\n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it):\n",
    "        # Sample from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Ensure proper dimensions\n",
    "        reward = reward.view(-1, 1)\n",
    "        done = done.view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (torch.randn_like(action) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target1(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, _ = self.critic1(state, action)\n",
    "        current_Q2, _ = self.critic2(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critics\n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward()\n",
    "        self.critic_optimizer1.step()\n",
    "\n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if total_it % policy_delay == 0:\n",
    "            # Compute actor loss\n",
    "            actor_action = self.actor(state)\n",
    "            Q1, _ = self.critic1(state, actor_action)\n",
    "            actor_loss = -Q1.mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "\n",
    "        return {\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def compute_parameter_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_policy(actor, env, num_episodes=10, device='cuda'):\n",
    "    \"\"\"Evaluate the policy for multiple drones without exploration noise\"\"\"\n",
    "    eval_rewards = []\n",
    "    eval_success = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        states, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        dones = {drone_name: False for drone_name in env.drone_names}\n",
    "        \n",
    "        while not all(dones.values()):\n",
    "            actions = {}\n",
    "            # Get actions for all active drones\n",
    "            for drone_name, state in states.items():\n",
    "                if not dones[drone_name]:\n",
    "                    with torch.no_grad():\n",
    "                        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                        actions[drone_name] = actor(state_tensor).cpu().numpy().flatten()\n",
    "            \n",
    "            # Step environment with all drone actions\n",
    "            next_states, rewards, terminateds, truncateds, infos = env.step(actions)\n",
    "            \n",
    "            # Update states and track rewards\n",
    "            episode_reward += np.mean(list(rewards.values()))  # Average reward across drones\n",
    "            for drone_name in env.drone_names:\n",
    "                if not dones[drone_name]:\n",
    "                    dones[drone_name] = terminateds[drone_name] or truncateds[drone_name]\n",
    "                    if infos[drone_name].get('success', False):\n",
    "                        eval_success += 1\n",
    "                        break  # Count only one success per episode\n",
    "            \n",
    "            states = next_states\n",
    "        \n",
    "        eval_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(eval_rewards),\n",
    "        'std_reward': np.std(eval_rewards),\n",
    "        'success_rate': eval_success / num_episodes,\n",
    "        'rewards': eval_rewards\n",
    "    }\n",
    "\n",
    "def run_training(env, obs_dim, act_dim, max_action, episodes=10000):\n",
    "    # Initialize environment and models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize TensorBoard writer with more descriptive name\n",
    "    current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join('logs', f'TD3_train_{current_time}')\n",
    "    train_writer = SummaryWriter(log_dir + '/train')\n",
    "    eval_writer = SummaryWriter(log_dir + '/eval')\n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "    \n",
    "    # Model initialization\n",
    "    actor = TD3Actor(obs_dim, act_dim, max_action).to(device)\n",
    "    critic1 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    critic2 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=3e-4)\n",
    "    critic_optimizer1 = optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "    critic_optimizer2 = optim.Adam(critic2.parameters(), lr=3e-4)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    total_steps = 0\n",
    "    current_stage = 0\n",
    "    best_rewards = {i: float('-inf') for i in range(9)}\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load('models/TD3/Model/mtp_model_main.pth')\n",
    "        actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        critic1.load_state_dict(checkpoint['critic1_state_dict'])\n",
    "        critic2.load_state_dict(checkpoint['critic2_state_dict'])\n",
    "        actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        critic_optimizer1.load_state_dict(checkpoint['critic1_optimizer_state_dict'])\n",
    "        critic_optimizer2.load_state_dict(checkpoint['critic2_optimizer_state_dict'])\n",
    "        \n",
    "        try:\n",
    "            total_steps = checkpoint['total_steps']\n",
    "            current_stage = checkpoint['stage']\n",
    "            best_rewards = checkpoint['best_rewards']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        print(f\"Loaded checkpoint successfully. Total steps: {total_steps}\")\n",
    "    except:\n",
    "        print(\"No checkpoint found, starting fresh\")\n",
    "\n",
    "    # Initialize or load replay buffer\n",
    "    try:\n",
    "        replay_buffer = ReplayBuffer.load('models/TD3/Replay_Buffer/mtp_replay_buffer.pkl')\n",
    "        print(\"Loaded buffer successfully\")\n",
    "    except:\n",
    "        replay_buffer = ReplayBuffer(max_size=1_000_000, obs_dim=obs_dim, act_dim=act_dim)\n",
    "        print(\"No buffer found, starting fresh\")\n",
    "    \n",
    "    trainer = TD3Trainer(\n",
    "        actor=actor,\n",
    "        critic1=critic1,\n",
    "        critic2=critic2,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_optimizer1=critic_optimizer1,\n",
    "        critic_optimizer2=critic_optimizer2,\n",
    "        max_action=max_action,\n",
    "        device=device\n",
    "    )\n",
    "    trainer._hard_update_targets()\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 256\n",
    "    warmup_steps = 10000\n",
    "    noise_std = 0.1\n",
    "    noise_clip = 0.5\n",
    "    policy_delay = 2\n",
    "    eval_freq = 1000  # Evaluate every 1000 episodes\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('models/TD3/Replay_Buffer', exist_ok=True)\n",
    "    os.makedirs('models/TD3/Model', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    def get_exploration_noise(action):\n",
    "        \"\"\"Return exploration noise as numpy array with correct shape\"\"\"\n",
    "        noise = np.random.normal(0, noise_std, size=action.shape)\n",
    "        return np.clip(noise, -noise_clip, noise_clip)\n",
    "    \n",
    "    def log_training_step(train_info, step):\n",
    "        # Log losses\n",
    "        train_writer.add_scalar('Loss/Critic1', train_info['critic_loss1'], step)\n",
    "        train_writer.add_scalar('Loss/Critic2', train_info['critic_loss2'], step)\n",
    "        if train_info['actor_loss'] is not None:\n",
    "            train_writer.add_scalar('Loss/Actor', train_info['actor_loss'], step)\n",
    "        \n",
    "        # Log gradient norms\n",
    "        train_writer.add_scalar('Gradients/Actor_Norm', compute_gradient_norm(actor), step)\n",
    "        train_writer.add_scalar('Gradients/Critic1_Norm', compute_gradient_norm(critic1), step)\n",
    "        train_writer.add_scalar('Gradients/Critic2_Norm', compute_gradient_norm(critic2), step)\n",
    "        \n",
    "        # Log parameter norms\n",
    "        train_writer.add_scalar('Parameters/Actor_Norm', compute_parameter_norm(actor), step)\n",
    "        train_writer.add_scalar('Parameters/Critic1_Norm', compute_parameter_norm(critic1), step)\n",
    "        train_writer.add_scalar('Parameters/Critic2_Norm', compute_parameter_norm(critic2), step)\n",
    "        \n",
    "        # Log learning rates\n",
    "        train_writer.add_scalar('LearningRate/Actor', actor_optimizer.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic1', critic_optimizer1.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic2', critic_optimizer2.param_groups[0]['lr'], step)\n",
    "    \n",
    "    def run_evaluation(episode):\n",
    "        # Create a separate environment for evaluation\n",
    "        eval_env = env.__class__()  # Assuming env has a constructor that takes no arguments\n",
    "        eval_env.training_stage = env.training_stage  # Sync the training stage\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_results = evaluate_policy(actor, eval_env, num_episodes=10, device=device)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        eval_writer.add_scalar('Eval/Mean_Reward', eval_results['mean_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Reward_Std', eval_results['std_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Success_Rate', eval_results['success_rate'], episode)\n",
    "        \n",
    "        # Log reward distribution\n",
    "        eval_writer.add_histogram('Eval/Reward_Distribution', \n",
    "                                torch.tensor(eval_results['rewards']), \n",
    "                                episode)\n",
    "        \n",
    "        eval_env.close()\n",
    "        return eval_results\n",
    "    \n",
    "    \n",
    "    # Modified reward tracking initialization\n",
    "    recent_rewards = deque(maxlen=100)  # Track recent episode rewards\n",
    "    stage_rewards = {i: [] for i in range(9)}  # Track rewards per stage\n",
    "    best_eval_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    # Initialize states and tracking\n",
    "\n",
    "    progress_bar = tqdm.tqdm(range(episodes), desc=\"Training\")\n",
    "    for episode in progress_bar:\n",
    "        current_episode_reward = 0  # Track current episode's reward sum\n",
    "        states, _ = env.reset()\n",
    "        dones = {drone_name: False for drone_name in env.drone_names}\n",
    "        episode_rewards_per_drone = {drone_name: 0 for drone_name in env.drone_names}\n",
    "        episodes_completed = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        while not dones:\n",
    "            actions = {}\n",
    "            # Log current stage\n",
    "            train_writer.add_scalar('Training/Current_Stage', current_stage, total_steps)\n",
    "        \n",
    "            # Select actions for each active drone\n",
    "            for drone_name in env.drone_names:\n",
    "                if not dones[drone_name]:\n",
    "                    state = states[drone_name]\n",
    "                    if total_steps < warmup_steps:\n",
    "                        actions[drone_name] = np.random.uniform(-max_action, max_action, size=act_dim)\n",
    "                        train_writer.add_scalar('Training/Exploration_Type', 0, total_steps)\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                            action = actor(state_tensor).cpu().numpy().flatten()\n",
    "                            noise = get_exploration_noise(torch.from_numpy(action))\n",
    "                            noisy_action = action + noise\n",
    "                            actions[drone_name] = np.clip(noisy_action, -max_action, max_action)\n",
    "                        \n",
    "                        train_writer.add_scalar('Training/Exploration_Type', 1, total_steps)\n",
    "                        train_writer.add_scalar('Training/Exploration_Noise', noise, total_steps)\n",
    "            \n",
    "            # Log action statistics\n",
    "            all_actions = np.array(list(actions.values()))  # Shape: (num_drones, action_dim)\n",
    "            train_writer.add_histogram('Actions/Distribution', all_actions, total_steps)\n",
    "            train_writer.add_scalar('Actions/Mean', np.mean(all_actions), total_steps)\n",
    "            train_writer.add_scalar('Actions/Std', np.std(all_actions), total_steps)\n",
    "            \n",
    "            # Step environment\n",
    "            next_states, rewards, terminateds, truncateds, info = env.step(actions)\n",
    "            \n",
    "            # Store transition\n",
    "            for drone_name in env.drone_names:\n",
    "                if not dones[drone_name]:\n",
    "                    replay_buffer.add(\n",
    "                        states[drone_name],\n",
    "                        actions[drone_name],\n",
    "                        rewards[drone_name],\n",
    "                        next_states[drone_name],\n",
    "                        float(terminateds[drone_name] or truncateds[drone_name])\n",
    "                    )\n",
    "                    current_episode_reward += rewards[drone_name]\n",
    "                    # If this drone is done, reset just this drone and count it as an episode\n",
    "                    if terminateds[drone_name] or truncateds[drone_name]:\n",
    "                        # Log the completed episode for this drone\n",
    "                        episode_rewards.append(current_episode_reward)\n",
    "                        recent_rewards.append(current_episode_reward)\n",
    "                        stage_rewards[current_stage].append(current_episode_reward)\n",
    "                        \n",
    "                        # Reset this drone's episode reward\n",
    "                        episode_rewards_per_drone[drone_name] = 0\n",
    "                        episodes_completed += 1\n",
    "                        dones[drone_name] = True\n",
    "            \n",
    "            # Update state and metrics\n",
    "            states.update(next_states)\n",
    "            for drone_name in env.drone_names:\n",
    "                if dones[drone_name]:\n",
    "                    states[drone_name] = env._get_observation(drone_name)\n",
    "                    dones[drone_name] = False\n",
    "\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Train agent\n",
    "            if total_steps > warmup_steps and len(replay_buffer.states) > batch_size:\n",
    "                train_info = trainer.train_step(\n",
    "                    replay_buffer=replay_buffer,\n",
    "                    batch_size=batch_size,\n",
    "                    noise_std=noise_std,\n",
    "                    noise_clip=noise_clip,\n",
    "                    policy_delay=policy_delay,\n",
    "                    total_it=total_steps,\n",
    "                    #action=list(actions.values())[0],\n",
    "                    #reward = list(rewards.values())[0],\n",
    "                    #noise = current_noise\n",
    "                )\n",
    "                \n",
    "                # Log training metrics\n",
    "                log_training_step(train_info, total_steps)\n",
    "            # Run evaluation periodically\n",
    "        if episodes_completed % eval_freq == 0:\n",
    "            eval_results = run_evaluation(episodes_completed)\n",
    "            \n",
    "            # Save best model based on evaluation\n",
    "            if eval_results['mean_reward'] > best_eval_reward:\n",
    "                best_eval_reward = eval_results['mean_reward']\n",
    "                torch.save({\n",
    "                    'actor_state_dict': actor.state_dict(),\n",
    "                    'critic1_state_dict': critic1.state_dict(),\n",
    "                    'critic2_state_dict': critic2.state_dict(),\n",
    "                    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                    'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                    'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                    'total_steps': total_steps,\n",
    "                    'episode': episodes_completed,\n",
    "                    'stage': current_stage,\n",
    "                    'eval_reward': best_eval_reward,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, 'models/TD3/Model/mtp_model_best_eval.pth')\n",
    "                \n",
    "        train_writer.add_scalar('Training/Current_Stage', env.training_stage, total_steps)\n",
    "        train_writer.add_scalar('Training/Episodes_Completed', episodes_completed, total_steps)\n",
    "        train_writer.add_scalar('Training/Buffer_Size', len(replay_buffer), total_steps)\n",
    "        \n",
    "        # Stage-specific metrics\n",
    "        if len(stage_rewards[current_stage]) >= 100:\n",
    "            # Convert to numpy array and ensure float type\n",
    "            stage_reward_array = np.array(stage_rewards[current_stage][-100:], dtype=np.float32)\n",
    "            current_avg_reward = float(np.mean(stage_reward_array))\n",
    "            \n",
    "            train_writer.add_scalar(f'Stage_{current_stage}/Average_100_Episodes', \n",
    "                                  current_avg_reward, \n",
    "                                  len(stage_rewards[current_stage]))\n",
    "            \n",
    "            if current_avg_reward > best_rewards[current_stage]:\n",
    "                best_rewards[current_stage] = current_avg_reward\n",
    "                train_writer.add_scalar(f'Stage_{current_stage}/Best_Average_Reward', \n",
    "                                      current_avg_reward, \n",
    "                                      len(stage_rewards[current_stage]))\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'stage': env.training_stage,\n",
    "            'episodes': episodes_completed,\n",
    "            'buffer': len(replay_buffer)\n",
    "        })\n",
    "        \n",
    "        # Check for training completion\n",
    "        if current_stage == 8 and env.success_count >= env.stage_threshold:\n",
    "            print(\"\\n=== Training completed successfully! ===\")\n",
    "            env.close()\n",
    "            print(\"Environment closed. Running final evaluation...\")\n",
    "            final_eval_results = run_evaluation(episodes_completed)\n",
    "            print(f\"Final evaluation results: {final_eval_results}\")\n",
    "            train_writer.close()\n",
    "            eval_writer.close()\n",
    "            break\n",
    "        \n",
    "        # Periodic checkpointing\n",
    "        if episodes_completed % 2500 == 0:\n",
    "            checkpoint_path = f'models/TD3/Model/mtp_backup_{episodes_completed}.pth'\n",
    "            torch.save({\n",
    "                'actor_state_dict': actor.state_dict(),\n",
    "                'critic1_state_dict': critic1.state_dict(),\n",
    "                'critic2_state_dict': critic2.state_dict(),\n",
    "                'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                'episode': episodes_completed,\n",
    "                'total_steps': total_steps,\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    return actor, critic1, critic2, current_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henhe\\AppData\\Local\\Temp\\ipykernel_93504\\2644628221.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('models/TD3/Model/mtp_model_main.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n",
      "Using device: cuda\n",
      "TensorBoard logs will be saved to: logs\\TD3_train_20250120-180707\n",
      "No checkpoint found, starting fresh\n",
      "No buffer found, starting fresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/10000 [00:04<11:47:45,  4.25s/it, stage=0, episodes=0, buffer=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_0.pth\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/10000 [00:07<10:19:16,  3.72s/it, stage=0, episodes=0, buffer=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_0.pth\n",
      "Connected!\n",
      "Client Ver:1 (Min Req: 1), Server Ver:1 (Min Req: 1)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m actor, critic1, critic2, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10_000\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 283\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(env, obs_dim, act_dim, max_action, episodes)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# Run evaluation periodically\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episodes_completed \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 283\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes_completed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Save best model based on evaluation\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m best_eval_reward:\n",
      "Cell \u001b[1;32mIn[22], line 169\u001b[0m, in \u001b[0;36mrun_training.<locals>.run_evaluation\u001b[1;34m(episode)\u001b[0m\n\u001b[0;32m    166\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mtraining_stage \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtraining_stage  \u001b[38;5;66;03m# Sync the training stage\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Log evaluation metrics\u001b[39;00m\n\u001b[0;32m    172\u001b[0m eval_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEval/Mean_Reward\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_reward\u001b[39m\u001b[38;5;124m'\u001b[39m], episode)\n",
      "Cell \u001b[1;32mIn[22], line 33\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(actor, env, num_episodes, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     32\u001b[0m             state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 33\u001b[0m             actions[drone_name] \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Step environment with all drone actions\u001b[39;00m\n\u001b[0;32m     36\u001b[0m next_states, rewards, terminateds, truncateds, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 229\u001b[0m, in \u001b[0;36mTD3Actor.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    228\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(state)\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_action \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\drone\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = MultiDroneCosysAirSimEnv()\n",
    "\n",
    "# Get environment dimensions\n",
    "obs_dim = env.observation_space.shape[0]  \n",
    "act_dim = env.action_space.shape[0]     \n",
    "max_action = float(env.action_space.high[0]) \n",
    "\n",
    "# Run training\n",
    "actor, critic1, critic2, rewards = run_training(\n",
    "    env=env,\n",
    "    obs_dim=obs_dim,\n",
    "    act_dim=act_dim,\n",
    "    max_action=max_action,\n",
    "    episodes= 10_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
