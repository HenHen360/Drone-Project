{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n",
      "Connected to AirSim!\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Main libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import cosysairsim as airsim\n",
    "from cosysairsim import MultirotorClient\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#General\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import logging\n",
    "from logging import handlers\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "client = airsim.MultirotorClient()\n",
    "client.confirmConnection()\n",
    "print(\"Connected to AirSim!\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultirotorClient()\n",
    "client.confirmConnection()\n",
    "client.enableApiControl(True)\n",
    "client.armDisarm(True)\n",
    "action=[0,0,1,0]\n",
    "max_angles = np.radians([30, 30, 45])  # Reduced pitch/roll angles\n",
    "pitch = np.clip(action[0] * max_angles[0], -max_angles[0], max_angles[0])\n",
    "roll = np.clip(action[1] * max_angles[1], -max_angles[1], max_angles[1])\n",
    "\n",
    "# Use exponential mapping for throttle to give more control in hover region\n",
    "throttle = (action[2])+1 / 2  # Centered around 0.5\n",
    "yaw_rate = np.clip(action[3] * max_angles[2], -max_angles[2], max_angles[2])\n",
    "\n",
    "# Add small time delay for stability\n",
    "client.moveByRollPitchYawrateThrottleAsync(\n",
    "    roll=roll,\n",
    "    pitch=pitch,\n",
    "    yaw_rate=yaw_rate,\n",
    "    throttle=throttle,\n",
    "    duration=1000  # Reduced duration for more frequent updates\n",
    ").join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train to keep in air\n",
    "\n",
    "class CosysAirSimEnv_Basic(gym.Env):\n",
    "    def __init__(self, training_stage=0):\n",
    "        super(CosysAirSimEnv_Basic, self).__init__()\n",
    "\n",
    "        self.client = MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(18,), dtype=np.float32)\n",
    "\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.episode = 0\n",
    "        self.wind_factor = 0.0\n",
    "        self.last_throttle = 0.6\n",
    "        self.success_count = 0\n",
    "        \n",
    "        self.success_threshold = 10\n",
    "        \n",
    "        self.training_stage = training_stage\n",
    "        self.setup_stage_params()\n",
    "        self.stage_progress = 0\n",
    "        self.stage_threshold = 25\n",
    "        \n",
    "        self.hover_survival_count = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.current_target = self._generate_new_target()\n",
    "        \n",
    "        self.all_time_min_distance = float('inf')\n",
    "        high = np.array([\n",
    "            1.0, 1.0, 1.0,      # normalized position\n",
    "            1.0, 1.0, 1.0,      # normalized velocity\n",
    "            1.0, 1.0, 1.0,      # normalized angular velocity\n",
    "            3.0, 3.0, 3.0,      # normalized acceleration (in G's)\n",
    "            1.0, 1.0, 1.0,      # normalized target position\n",
    "            1.0,                # normalized distance\n",
    "            1.0,                 # normalized height error\n",
    "            1.0\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def setup_stage_params(self):\n",
    "        self.params = {\n",
    "            0: {  # Hovering stage\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 5.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'hover',\n",
    "                'height_range': [-2.0, -1.5],  # Desired hover height\n",
    "                'target_radius': 0.5\n",
    "            },\n",
    "            1: {  # Near vertical movement\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 8.0],\n",
    "                'max_steps': 250,\n",
    "                'target_type': 'vertical',\n",
    "                'height_range': [-4.0, -1.0],\n",
    "                'target_radius': 0.3\n",
    "            },\n",
    "            2: {  # Close-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 5.0\n",
    "            },\n",
    "            3: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 400,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 10.0\n",
    "            },\n",
    "            4: {  # Long-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0\n",
    "            },\n",
    "            5: {  # Extended range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [30.0, 30.0, 15.0],  # Expanded area\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 25.0  # Increased maximum distance\n",
    "            },\n",
    "            6: {  # Recovery training\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 50  # Apply random action every 50 steps\n",
    "            },\n",
    "            7: {  # Moderate wind\n",
    "                'wind': 0.5,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,\n",
    "                'max_target_dist': 25.0\n",
    "            },\n",
    "            8: {  # Strong wind\n",
    "                'wind': 1.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,  # Slightly larger radius due to strong wind\n",
    "                'max_target_dist': 25.0\n",
    "            }\n",
    "        }\n",
    "        self.current_params = self.params[self.training_stage]\n",
    "        \n",
    "    def _generate_new_target(self):\n",
    "        \"\"\"Internal method to generate a new target based on current parameters\"\"\"\n",
    "        params = self.current_params\n",
    "        \n",
    "        if params['target_type'] == 'hover':\n",
    "            return np.array([0.0, 0.0, -1.75])\n",
    "            \n",
    "        elif params['target_type'] == 'vertical':\n",
    "            height = np.random.uniform(*params['height_range'])\n",
    "            return np.array([0.0, 0.0, height])\n",
    "            \n",
    "        elif params['target_type'] == 'horizontal_near':\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            dist = np.random.uniform(2.0, params['max_target_dist'])\n",
    "            return np.array([\n",
    "                dist * np.cos(angle),\n",
    "                dist * np.sin(angle),\n",
    "                -2.0\n",
    "            ])\n",
    "            \n",
    "        elif params['target_type'] in ['free_near', 'free']:\n",
    "            while True:\n",
    "                point = np.random.uniform(\n",
    "                    low=[-params['max_target_dist'], -params['max_target_dist'], -4.0],\n",
    "                    high=[params['max_target_dist'], params['max_target_dist'], -1.0],\n",
    "                    size=(3,)\n",
    "                )\n",
    "                if np.linalg.norm(point - self._get_current_position()) > 2.0:\n",
    "                    return point\n",
    "\n",
    "    def _get_target(self):\n",
    "        \"\"\"Return the current target\"\"\"\n",
    "        return self.current_target\n",
    "    \n",
    "    def _get_distance_target(self):\n",
    "        return np.linalg.norm(self._get_current_position() - self._get_target())\n",
    "        \n",
    "    def _get_current_position(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        pos = kinematics.position\n",
    "\n",
    "        return np.array([pos.x_val, pos.y_val, pos.z_val], dtype=np.float32)\n",
    "\n",
    "    def _get_current_velocity(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        vel = kinematics.linear_velocity\n",
    "\n",
    "        return np.array([vel.x_val, vel.y_val, vel.z_val], dtype=np.float32)\n",
    "    \n",
    "    def _get_imu_data(self):\n",
    "        imu_data = self.client.getImuData()\n",
    "        # IMU typically gives angular velocity and linear acceleration\n",
    "        # Store them as [wx, wy, wz, ax, ay, az]\n",
    "        ang_vel = imu_data.angular_velocity\n",
    "        lin_acc = imu_data.linear_acceleration\n",
    "        return np.array([\n",
    "            ang_vel.x_val, ang_vel.y_val, ang_vel.z_val,\n",
    "            lin_acc.x_val, lin_acc.y_val, lin_acc.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_mag_data(self):\n",
    "        mag_data = self.client.getMagnetometerData()\n",
    "        # magnetometer.x, .y, .z\n",
    "        return np.array([\n",
    "            mag_data.magnetic_field_body.x_val,\n",
    "            mag_data.magnetic_field_body.y_val,\n",
    "            mag_data.magnetic_field_body.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_gps_data(self):\n",
    "        gps_data = self.client.getGpsData()\n",
    "        # gps_data.gnss.geo_point.latitude, .longitude, .altitude\n",
    "        return np.array([\n",
    "            gps_data.gnss.geo_point.latitude,\n",
    "            gps_data.gnss.geo_point.longitude,\n",
    "            gps_data.gnss.geo_point.altitude\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_baro_data(self):\n",
    "        baro_data = self.client.getBarometerData()\n",
    "        # baro_data.altitude, baro_data.pressure, baro_data.qnh\n",
    "        # Just use altitude\n",
    "        return np.array([baro_data.altitude], dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        #Get and standardize observations.\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # We'll ignore GPS and magnetometer data as they're less relevant for this task\n",
    "        # and could add noise to the learning process\n",
    "\n",
    "        # Standardize position: Scale by area bounds\n",
    "        normalized_pos = position / np.array(self.current_params['area'])\n",
    "\n",
    "        # Standardize velocity: Most drones operate within -10 to 10 m/s range\n",
    "        normalized_vel = np.clip(velocity / 10.0, -1.0, 1.0)\n",
    "\n",
    "        # Standardize IMU data\n",
    "        angular_vel = imu_data[:3] / 10.0  # Angular velocity (rad/s)\n",
    "        linear_acc = imu_data[3:6] / 9.81  # Linear acceleration (normalize by g)\n",
    "\n",
    "        # Standardize target and distance\n",
    "        normalized_target = target / np.array(self.current_params['area'])\n",
    "        normalized_distance = distance / np.linalg.norm(self.current_params['area'])\n",
    "\n",
    "        # Height error (important for hovering)\n",
    "        height_error = (position[2] - target[2]) / self.current_params['area'][2]\n",
    "\n",
    "        # Combine all observations\n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]   - Normalized position (x, y, z)\n",
    "            normalized_vel,        # [3:6]   - Normalized velocity (vx, vy, vz)\n",
    "            angular_vel,          # [6:9]   - Normalized angular velocity (wx, wy, wz)\n",
    "            linear_acc,          # [9:12]  - Normalized linear acceleration (ax, ay, az)\n",
    "            normalized_target,    # [12:15] - Normalized target position\n",
    "            [normalized_distance],# [15]    - Normalized distance to target\n",
    "            [height_error]       # [16]    - Normalized height error\n",
    "        ])\n",
    "        # Clip all values to ensure they stay within a reasonable range\n",
    "        observation = np.clip(observation, -5.0, 5.0)\n",
    "        \n",
    "        return observation.astype(np.float32)\"\"\"\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # Normalize position relative to the current stage's boundaries\n",
    "        area_bounds = np.array(self.current_params['area'])\n",
    "        normalized_pos = np.clip(position / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize velocity (typical drone speeds rarely exceed ±10 m/s)\n",
    "        velocity_scale = np.array([10.0, 10.0, 5.0])  # Less vertical velocity range\n",
    "        normalized_vel = np.clip(velocity / velocity_scale, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize IMU data\n",
    "        # Angular velocity (typically ±5 rad/s)\n",
    "        angular_vel = np.clip(imu_data[:3] / 5.0, -1.0, 1.0)\n",
    "        # Linear acceleration (normalize by g ≈ 9.81 m/s²)\n",
    "        linear_acc = np.clip(imu_data[3:6] / 9.81, -3.0, 3.0)  # Allow for higher G-forces\n",
    "        \n",
    "        # Normalize target position\n",
    "        normalized_target = np.clip(target / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize distance relative to maximum possible distance in current area\n",
    "        max_possible_distance = np.linalg.norm(area_bounds)\n",
    "        normalized_distance = np.clip(distance / max_possible_distance, 0.0, 1.0)\n",
    "        \n",
    "        # Height error normalization\n",
    "        max_height_error = area_bounds[2]\n",
    "        height_error = (position[2] - target[2]) / max_height_error\n",
    "        normalized_height_error = np.clip(height_error, -1.0, 1.0)\n",
    "\n",
    "        relative_pos = (target - position) / np.array(self.current_params['area'])\n",
    "        \n",
    "        target_direction = (target - position) / (np.linalg.norm(target - position) + 1e-6)\n",
    "        relative_vel = np.dot(velocity, target_direction)\n",
    "        \n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]\n",
    "            normalized_vel,        # [3:6]\n",
    "            angular_vel,          # [6:9]\n",
    "            linear_acc,          # [9:12]\n",
    "            relative_pos,        # [12:15]\n",
    "            [normalized_distance],# [15]\n",
    "            [height_error],      # [16]\n",
    "            [relative_vel]       # [17] - Added velocity towards target\n",
    "        ])\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _apply_wind(self):\n",
    "        \"\"\"Applies a simulated wind force to the drone.\"\"\"\n",
    "        wind_x = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_y = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_z = np.random.uniform(-self.wind_factor / 2, self.wind_factor / 2)\n",
    "        self.client.simSetWind(airsim.Vector3r(wind_x, wind_y, wind_z))\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # Reset the drone\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        self.client.takeoffAsync().join()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.episode += 1\n",
    "        self._apply_wind()\n",
    "        \n",
    "        if not hasattr(self, 'current_target') or self.current_target is None:\n",
    "            self._get_target()  # This will generate a new target\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        \n",
    "        if self.training_stage == 6:\n",
    "            if self.current_step % self.current_params['recovery_interval'] == 0:\n",
    "                # Apply random action\n",
    "                random_action = np.random.uniform(-1, 1, size=4)\n",
    "                self.recovery_action_timer = 10  # Recovery period\n",
    "                action = random_action\n",
    "            elif self.recovery_action_timer > 0:\n",
    "                self.recovery_action_timer -= 1\n",
    "        # Process controls - separate throttle handling\n",
    "        max_angles = np.radians([30, 30, 45])  # Reduced pitch/roll angles\n",
    "        \n",
    "        pitch = np.clip(action[0] * max_angles[0], -max_angles[0], max_angles[0])\n",
    "        roll = np.clip(action[1] * max_angles[1], -max_angles[1], max_angles[1])\n",
    "        \n",
    "        # Use exponential mapping for throttle to give more control in hover region\n",
    "        throttle = (action[2])*0.4 + 0.6  # Centered around 0.5\n",
    "        yaw_rate = np.clip(action[3] * max_angles[2], -max_angles[2], max_angles[2])\n",
    "        # Add small time delay for stability\n",
    "        self.client.moveByRollPitchYawrateThrottleAsync(\n",
    "            roll=roll,\n",
    "            pitch=pitch,\n",
    "            yaw_rate=yaw_rate,\n",
    "            throttle=throttle,\n",
    "            duration=0.2  # Reduced duration for more frequent updates\n",
    "        ).join()\n",
    "        \n",
    "        self._apply_wind()\n",
    "        observation = self._get_observation()\n",
    "        reward, terminated = self._calculate_reward()\n",
    "        \n",
    "        truncated = self.current_step >= self.current_params['max_steps']\n",
    "        #if truncated:\n",
    "        #    reward += 10\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:\n",
    "            # Increment the counter.\n",
    "            if truncated:\n",
    "                self.hover_survival_count += 1\n",
    "                reward += 50\n",
    "\n",
    "                if self.hover_survival_count >= self.num_consecutive_survivals_needed:\n",
    "                    self.training_stage = 1\n",
    "                    self.setup_stage_params()\n",
    "                    self.hover_survival_count = 0\n",
    "                    print(\n",
    "                        f\"=== Moved to stage {self.training_stage} after \"\n",
    "                        f\"{self.num_consecutive_survivals_needed} consecutive survival episodes. ===\"\n",
    "                    )\n",
    "        \"\"\"\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "    def _calculate_reward(self):\n",
    "        pos = self._get_current_position()\n",
    "        vel = self._get_current_velocity()\n",
    "        angular_vel = self._get_imu_data()[:3]\n",
    "        target = self._get_target()\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        reward_scale = 0.1\n",
    "        reward = 0.0\n",
    "        # Immediate failure conditions (keep these)\n",
    "        if self.client.simGetCollisionInfo().has_collided:\n",
    "            reward -= 100*reward_scale\n",
    "            return reward, True\n",
    "        if any(abs(p) > a for p, a in zip(pos, self.current_params['area'])):\n",
    "            reward -= 50*reward_scale\n",
    "            return reward, True\n",
    "        # Main distance reward - smoother gradient\n",
    "        distance_reward = -distance + 1  # Linear scaling for more consistent gradient\n",
    "        \n",
    "        \"\"\"\n",
    "        # Stability penalty - much gentler\n",
    "        stability_penalty = (\n",
    "            0.05 * np.linalg.norm(angular_vel) +  # Reduced rotation penalty\n",
    "            0.02 * np.linalg.norm(vel)            # Reduced velocity penalty\n",
    "        )\n",
    "        reward -= stability_penalty\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:  # Hovering\n",
    "            # Pure hovering - focus on stability\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "            \n",
    "        elif self.training_stage == 1:  # Vertical movement\n",
    "            # Reward vertical progress toward target\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (2.0 * np.exp(-vertical_distance))*reward_scale\n",
    "            \n",
    "            # Penalize horizontal drift more strongly\n",
    "            horizontal_drift = np.linalg.norm(pos[:2] - target[:2])\n",
    "            reward -= (0.5 * horizontal_drift)*reward_scale\n",
    "            \n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "        elif self.training_stage == 2:  # Close-range horizontal\n",
    "            # Reward horizontal progress\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (3.0 * progress)*reward_scale  # Stronger reward for deliberate movement\n",
    "            self._prev_distance = distance\n",
    "            \n",
    "            # Height stability becomes secondary but still important\n",
    "            reward += (distance_reward)*reward_scale\n",
    "            \n",
    "        elif self.training_stage >= 3:  # Medium-range movement\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (2.0 * progress)*reward_scale\n",
    "            self._prev_distance = distance\n",
    "            reward += (distance_reward)*reward_scale\n",
    "        # Success condition\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance < success_radius:\n",
    "            bonus = 0\n",
    "            bonus += (25.0)*reward_scale\n",
    "            self.consecutive_success += 1\n",
    "            \n",
    "            if self.consecutive_success % self.success_threshold == 0:\n",
    "                self.current_target = self._generate_new_target()\n",
    "                bonus += 50.0*reward_scale  \n",
    "                self.success_count += 1\n",
    "        \n",
    "            if self.success_count >= self.stage_threshold:\n",
    "                self.training_stage = min(8, self.training_stage + 1)\n",
    "                print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "                self.success_count = 0\n",
    "                self.setup_stage_params()\n",
    "                \n",
    "            return reward, True\n",
    "        return reward, False\n",
    "\n",
    "    def close(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "        self.client.armDisarm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train to keep in air\n",
    "\n",
    "class CosysAirSimEnv_Velocity(gym.Env):\n",
    "    def __init__(self, training_stage=0):\n",
    "        super(CosysAirSimEnv_Velocity, self).__init__()\n",
    "\n",
    "        self.client = MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(18,), dtype=np.float32)\n",
    "\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.episode = 0\n",
    "        self.wind_factor = 0.0\n",
    "        self.last_throttle = 0.6\n",
    "        self.success_count = 0\n",
    "        \n",
    "        self.success_threshold = 10\n",
    "        \n",
    "        self.training_stage = training_stage\n",
    "        self.setup_stage_params()\n",
    "        self.stage_progress = 0\n",
    "        self.stage_threshold = 25\n",
    "        \n",
    "        self.hover_survival_count = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.current_target = self._generate_new_target()\n",
    "        \n",
    "        self.all_time_min_distance = float('inf')\n",
    "        high = np.array([\n",
    "            1.0, 1.0, 1.0,      # normalized position\n",
    "            1.0, 1.0, 1.0,      # normalized velocity\n",
    "            1.0, 1.0, 1.0,      # normalized angular velocity\n",
    "            3.0, 3.0, 3.0,      # normalized acceleration (in G's)\n",
    "            1.0, 1.0, 1.0,      # normalized target position\n",
    "            1.0,                # normalized distance\n",
    "            1.0,                 # normalized height error\n",
    "            1.0\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def setup_stage_params(self):\n",
    "        self.params = {\n",
    "            0: {  # Hovering stage\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 5.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'hover',\n",
    "                'height_range': [-2.0, -1.5],  # Desired hover height\n",
    "                'target_radius': 0.5\n",
    "            },\n",
    "            1: {  # Near vertical movement\n",
    "                'wind': 0.0,\n",
    "                'area': [5.0, 5.0, 8.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'vertical',\n",
    "                'height_range': [-4.0, -1.0],\n",
    "                'target_radius': 0.3\n",
    "            },\n",
    "            2: {  # Close-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 5.0\n",
    "            },\n",
    "            3: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 400,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 10.0\n",
    "            },\n",
    "            4: {  # Long-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0\n",
    "            },\n",
    "            5: {  # Extended range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [30.0, 30.0, 15.0],  # Expanded area\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 25.0  # Increased maximum distance\n",
    "            },\n",
    "            6: {  # Recovery training\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 50  # Apply random action every 50 steps\n",
    "            },\n",
    "            7: {  # Moderate wind\n",
    "                'wind': 0.5,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,\n",
    "                'max_target_dist': 25.0\n",
    "            },\n",
    "            8: {  # Strong wind\n",
    "                'wind': 1.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,  # Slightly larger radius due to strong wind\n",
    "                'max_target_dist': 25.0\n",
    "            }\n",
    "        }\n",
    "        self.current_params = self.params[self.training_stage]\n",
    "        \n",
    "    def _generate_new_target(self):\n",
    "        \"\"\"Internal method to generate a new target based on current parameters\"\"\"\n",
    "        params = self.current_params\n",
    "        \n",
    "        if params['target_type'] == 'hover':\n",
    "            return np.array([0.0, 0.0, -1.75])\n",
    "            \n",
    "        elif params['target_type'] == 'vertical':\n",
    "            height = np.random.uniform(*params['height_range'])\n",
    "            return np.array([0.0, 0.0, height])\n",
    "            \n",
    "        elif params['target_type'] == 'horizontal_near':\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            dist = np.random.uniform(2.0, params['max_target_dist'])\n",
    "            return np.array([\n",
    "                dist * np.cos(angle),\n",
    "                dist * np.sin(angle),\n",
    "                -2.0\n",
    "            ])\n",
    "            \n",
    "        elif params['target_type'] in ['free_near', 'free']:\n",
    "            while True:\n",
    "                point = np.random.uniform(\n",
    "                    low=[-params['max_target_dist'], -params['max_target_dist'], -4.0],\n",
    "                    high=[params['max_target_dist'], params['max_target_dist'], -1.0],\n",
    "                    size=(3,)\n",
    "                )\n",
    "                if np.linalg.norm(point - self._get_current_position()) > 2.0:\n",
    "                    return point\n",
    "\n",
    "    def _get_target(self):\n",
    "        \"\"\"Return the current target\"\"\"\n",
    "        return self.current_target\n",
    "    \n",
    "    def _get_distance_target(self):\n",
    "        return np.linalg.norm(self._get_current_position() - self._get_target())\n",
    "        \n",
    "    def _get_current_position(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        pos = kinematics.position\n",
    "\n",
    "        return np.array([pos.x_val, pos.y_val, pos.z_val], dtype=np.float32)\n",
    "\n",
    "    def _get_current_velocity(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        vel = kinematics.linear_velocity\n",
    "\n",
    "        return np.array([vel.x_val, vel.y_val, vel.z_val], dtype=np.float32)\n",
    "    \n",
    "    def _get_imu_data(self):\n",
    "        imu_data = self.client.getImuData()\n",
    "        # IMU typically gives angular velocity and linear acceleration\n",
    "        # We'll store them as [wx, wy, wz, ax, ay, az]\n",
    "        ang_vel = imu_data.angular_velocity\n",
    "        lin_acc = imu_data.linear_acceleration\n",
    "        return np.array([\n",
    "            ang_vel.x_val, ang_vel.y_val, ang_vel.z_val,\n",
    "            lin_acc.x_val, lin_acc.y_val, lin_acc.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_mag_data(self):\n",
    "        mag_data = self.client.getMagnetometerData()\n",
    "        # magnetometer.x, .y, .z\n",
    "        return np.array([\n",
    "            mag_data.magnetic_field_body.x_val,\n",
    "            mag_data.magnetic_field_body.y_val,\n",
    "            mag_data.magnetic_field_body.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_gps_data(self):\n",
    "        gps_data = self.client.getGpsData()\n",
    "        # gps_data.gnss.geo_point.latitude, .longitude, .altitude\n",
    "        return np.array([\n",
    "            gps_data.gnss.geo_point.latitude,\n",
    "            gps_data.gnss.geo_point.longitude,\n",
    "            gps_data.gnss.geo_point.altitude\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_baro_data(self):\n",
    "        baro_data = self.client.getBarometerData()\n",
    "        # baro_data.altitude, baro_data.pressure, baro_data.qnh\n",
    "        # We'll just use altitude\n",
    "        return np.array([baro_data.altitude], dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        #Get and standardize observations.\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # We'll ignore GPS and magnetometer data as they're less relevant for this task\n",
    "        # and could add noise to the learning process\n",
    "\n",
    "        # Standardize position: Scale by area bounds\n",
    "        normalized_pos = position / np.array(self.current_params['area'])\n",
    "\n",
    "        # Standardize velocity: Most drones operate within -10 to 10 m/s range\n",
    "        normalized_vel = np.clip(velocity / 10.0, -1.0, 1.0)\n",
    "\n",
    "        # Standardize IMU data\n",
    "        angular_vel = imu_data[:3] / 10.0  # Angular velocity (rad/s)\n",
    "        linear_acc = imu_data[3:6] / 9.81  # Linear acceleration (normalize by g)\n",
    "\n",
    "        # Standardize target and distance\n",
    "        normalized_target = target / np.array(self.current_params['area'])\n",
    "        normalized_distance = distance / np.linalg.norm(self.current_params['area'])\n",
    "\n",
    "        # Height error (important for hovering)\n",
    "        height_error = (position[2] - target[2]) / self.current_params['area'][2]\n",
    "\n",
    "        # Combine all observations\n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]   - Normalized position (x, y, z)\n",
    "            normalized_vel,        # [3:6]   - Normalized velocity (vx, vy, vz)\n",
    "            angular_vel,          # [6:9]   - Normalized angular velocity (wx, wy, wz)\n",
    "            linear_acc,          # [9:12]  - Normalized linear acceleration (ax, ay, az)\n",
    "            normalized_target,    # [12:15] - Normalized target position\n",
    "            [normalized_distance],# [15]    - Normalized distance to target\n",
    "            [height_error]       # [16]    - Normalized height error\n",
    "        ])\n",
    "        # Clip all values to ensure they stay within a reasonable range\n",
    "        observation = np.clip(observation, -5.0, 5.0)\n",
    "        \n",
    "        return observation.astype(np.float32)\"\"\"\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # Normalize position relative to the current stage's boundaries\n",
    "        area_bounds = np.array(self.current_params['area'])\n",
    "        normalized_pos = np.clip(position / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize velocity (typical drone speeds rarely exceed ±10 m/s)\n",
    "        velocity_scale = np.array([10.0, 10.0, 5.0])  # Less vertical velocity range\n",
    "        normalized_vel = np.clip(velocity / velocity_scale, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize IMU data\n",
    "        # Angular velocity (typically ±5 rad/s)\n",
    "        angular_vel = np.clip(imu_data[:3] / 5.0, -1.0, 1.0)\n",
    "        # Linear acceleration (normalize by g ≈ 9.81 m/s²)\n",
    "        linear_acc = np.clip(imu_data[3:6] / 9.81, -3.0, 3.0)  # Allow for higher G-forces\n",
    "        \n",
    "        # Normalize target position\n",
    "        normalized_target = np.clip(target / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize distance relative to maximum possible distance in current area\n",
    "        max_possible_distance = np.linalg.norm(area_bounds)\n",
    "        normalized_distance = np.clip(distance / max_possible_distance, 0.0, 1.0)\n",
    "        \n",
    "        # Height error normalization\n",
    "        max_height_error = area_bounds[2]\n",
    "        height_error = (position[2] - target[2]) / max_height_error\n",
    "        normalized_height_error = np.clip(height_error, -1.0, 1.0)\n",
    "\n",
    "        relative_pos = (target - position) / np.array(self.current_params['area'])\n",
    "        \n",
    "        target_direction = (target - position) / (np.linalg.norm(target - position) + 1e-6)\n",
    "        relative_vel = np.dot(velocity, target_direction)\n",
    "        \n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]\n",
    "            normalized_vel,        # [3:6]\n",
    "            angular_vel,          # [6:9]\n",
    "            linear_acc,          # [9:12]\n",
    "            relative_pos,        # [12:15]\n",
    "            [normalized_distance],# [15]\n",
    "            [height_error],      # [16]\n",
    "            [relative_vel]       # [17] - Added velocity towards target\n",
    "        ])\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _apply_wind(self):\n",
    "        \"\"\"Applies a simulated wind force to the drone.\"\"\"\n",
    "        wind_x = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_y = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_z = np.random.uniform(-self.wind_factor / 2, self.wind_factor / 2)\n",
    "        self.client.simSetWind(airsim.Vector3r(wind_x, wind_y, wind_z))\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # Reset the drone\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        self.client.takeoffAsync().join()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.episode += 1\n",
    "        self._apply_wind()\n",
    "        \n",
    "        if not hasattr(self, 'current_target') or self.current_target is None:\n",
    "            self._get_target()  # This will generate a new target\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        \n",
    "        if self.training_stage == 6:\n",
    "            if self.current_step % self.current_params['recovery_interval'] == 0:\n",
    "                # Apply random action\n",
    "                random_action = np.random.uniform(-1, 1, size=4)\n",
    "                self.recovery_action_timer = 10  # Recovery period\n",
    "                action = random_action\n",
    "            elif self.recovery_action_timer > 0:\n",
    "                self.recovery_action_timer -= 1\n",
    "        # Process controls - separate throttle handling\n",
    "        max_vx, max_vy, max_vz = 3.0, 3.0, 3.0   # [m/s]\n",
    "        max_yaw_deg = 45.0                      # [deg/s], or you can do rad/s\n",
    "\n",
    "        vx = float(action[0]) * max_vx\n",
    "        vy = float(action[1]) * max_vy\n",
    "        vz = float(action[2]) * max_vz\n",
    "        yaw_rate_deg = float(action[3]) * max_yaw_deg\n",
    "        \n",
    "        yaw_mode = airsim.YawMode(is_rate=True, yaw_or_rate=yaw_rate_deg)\n",
    "\n",
    "        # Move by velocity in BODY frame\n",
    "        self.client.moveByVelocityBodyFrameAsync(\n",
    "            vx=vx,\n",
    "            vy=vy,\n",
    "            vz=vz,\n",
    "            duration=0.25,  # small step\n",
    "            drivetrain=airsim.DrivetrainType.MaxDegreeOfFreedom,\n",
    "            yaw_mode=yaw_mode\n",
    "        ).join()\n",
    "        \n",
    "        self._apply_wind()\n",
    "        observation = self._get_observation()\n",
    "        reward, terminated = self._calculate_reward()\n",
    "        \n",
    "        truncated = self.current_step >= self.current_params['max_steps']\n",
    "        #if truncated:\n",
    "        #    reward += 10\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:\n",
    "            # If we ended the episode by hitting max_steps (truncate) WITHOUT crashing (terminated=False),\n",
    "            # then we survived. Increment the counter.\n",
    "            if truncated:\n",
    "                self.hover_survival_count += 1\n",
    "                reward += 50\n",
    "\n",
    "                # If we've survived enough times in a row, move to stage 1\n",
    "                if self.hover_survival_count >= self.num_consecutive_survivals_needed:\n",
    "                    self.training_stage = 1\n",
    "                    self.setup_stage_params()\n",
    "                    self.hover_survival_count = 0\n",
    "                    print(\n",
    "                        f\"=== Moved to stage {self.training_stage} after \"\n",
    "                        f\"{self.num_consecutive_survivals_needed} consecutive survival episodes. ===\"\n",
    "                    )\n",
    "        \"\"\"\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "    def _calculate_reward(self):\n",
    "        pos = self._get_current_position()\n",
    "        vel = self._get_current_velocity()\n",
    "        angular_vel = self._get_imu_data()[:3]\n",
    "        target = self._get_target()\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        reward_scale = 0.1\n",
    "        reward = 0.0\n",
    "        # Immediate failure conditions (keep these)\n",
    "        if self.client.simGetCollisionInfo().has_collided:\n",
    "            reward -= 100*reward_scale\n",
    "            return reward, True\n",
    "        if any(abs(p) > a for p, a in zip(pos, self.current_params['area'])):\n",
    "            reward -= 50*reward_scale\n",
    "            return reward, True\n",
    "        # Main distance reward - smoother gradient\n",
    "        distance_reward = -distance+1  # Linear scaling for more consistent gradient\n",
    "        \n",
    "        \"\"\"\n",
    "        # Stability penalty - much gentler\n",
    "        stability_penalty = (\n",
    "            0.05 * np.linalg.norm(angular_vel) +  # Reduced rotation penalty\n",
    "            0.02 * np.linalg.norm(vel)            # Reduced velocity penalty\n",
    "        )\n",
    "        reward -= stability_penalty\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:  # Hovering\n",
    "            # Pure hovering - focus on stability\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "            \n",
    "        elif self.training_stage == 1:  # Vertical movement\n",
    "            # Reward vertical progress toward target\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (2.0 * np.exp(-vertical_distance))*reward_scale\n",
    "            \n",
    "            # Penalize horizontal drift more strongly\n",
    "            horizontal_drift = np.linalg.norm(pos[:2] - target[:2])\n",
    "            reward -= (0.5 * horizontal_drift)*reward_scale\n",
    "            \n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "        elif self.training_stage == 2:  # Close-range horizontal\n",
    "            # Reward horizontal progress\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (3.0 * progress)*reward_scale  # Stronger reward for deliberate movement\n",
    "            self._prev_distance = distance\n",
    "            \n",
    "            # Height stability becomes secondary but still important\n",
    "            reward += (distance_reward)*reward_scale\n",
    "            \n",
    "        elif self.training_stage >= 3:  # Medium-range movement\n",
    "            # Balance distance and control\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (2.0 * progress)*reward_scale\n",
    "            self._prev_distance = distance\n",
    "            reward += (distance_reward)*reward_scale\n",
    "        # Success condition\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance < success_radius:\n",
    "            bonus = 0\n",
    "            bonus += (25.0)*reward_scale\n",
    "            self.consecutive_success += 1\n",
    "            \n",
    "            if self.consecutive_success % self.success_threshold == 0:\n",
    "                self.current_target = self._generate_new_target()\n",
    "                bonus += 50.0*reward_scale  \n",
    "                self.success_count += 1\n",
    "        \n",
    "            if self.success_count >= self.stage_threshold:\n",
    "                self.training_stage = min(8, self.training_stage + 1)\n",
    "                print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "                self.success_count = 0\n",
    "                self.setup_stage_params()\n",
    "                \n",
    "            return reward, True\n",
    "        return reward, False\n",
    "\n",
    "    def close(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "        self.client.armDisarm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO, TD3\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\stable_baselines3\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_system_info\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPG\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\stable_baselines3\\a2c\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m A2C\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ma2c\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CnnPolicy, MlpPolicy, MultiInputPolicy\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA2C\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuffers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RolloutBuffer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mon_policy_algorithm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OnPolicyAlgorithm\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:16\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_action_dim, get_obs_shape\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype_aliases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     DictReplayBufferSamples,\n\u001b[0;32m     12\u001b[0m     DictRolloutBufferSamples,\n\u001b[0;32m     13\u001b[0m     ReplayBufferSamples,\n\u001b[0;32m     14\u001b[0m     RolloutBufferSamples,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_device\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VecNormalize\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Check memory used by replay buffer when possible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Check if tensorboard is available for pytorch\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     SummaryWriter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mload_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\lazy.py:97\u001b[0m, in \u001b[0;36m_memoize.<locals>.wrapper\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mget(arg, nothing) \u001b[38;5;129;01mis\u001b[39;00m nothing:\n\u001b[1;32m---> 97\u001b[0m             cache[arg] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[arg]\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\lazy.py:50\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.load_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:45\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensorflow\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:99\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils_exp\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m internal\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\__init__.py:28\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ragged Tensors.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis package defines ops for manipulating ragged tensors (`tf.RaggedTensor`),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mAPI docstring: tensorflow.ragged\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:3149\u001b[0m\n\u001b[0;32m   3144\u001b[0m RaggedOrDense \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[Ragged, core_types\u001b[38;5;241m.\u001b[39mTensorLike]\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;66;03m# RaggedTensor must import ragged_ops to ensure that all dispatched ragged ops\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;66;03m# are registered. Ragged ops import RaggedTensor, so import at bottom of the\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;66;03m# file to avoid a partially-initialized module error.\u001b[39;00m\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_ops  \u001b[38;5;66;03m# pylint: disable=unused-import, g-bad-import-order, g-import-not-at-top\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_ops.py:27\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Import all modules in the `ragged` package that define exported symbols.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mAdditional, import ragged_dispatch (which has the side-effect of registering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mcircular dependencies.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_array_ops\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_batch_gather_ops\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_array_ops.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_ops\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_flow_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_ragged_array_ops\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\ops\\data_flow_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python_io\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\python_io.py:23\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"Python functions for directly manipulating TFRecord-formatted files.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mAPI docstring: tensorflow.python_io\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\tf_record.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"For reading and writing TFRecords files.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_record_io\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, TD3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "def train_sb3():\n",
    "    # 1) Create the environment\n",
    "    env = CosysAirSimEnv_Basic(training_stage=0)\n",
    "    \n",
    "    # 2) Wrap in a VecEnv (SB3 requires vectorized envs)\n",
    "    vec_env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    log_dir = \"tb_logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # 3) Instantiate the RL model (PPO, SAC, TD3, etc.)\n",
    "    # For continuous actions, PPO or SAC are popular choices\n",
    "    model = TD3(\n",
    "        policy=\"MlpPolicy\", \n",
    "        env=vec_env, \n",
    "        learning_rate=0.01, \n",
    "        buffer_size=1000000, \n",
    "        learning_starts=100, \n",
    "        batch_size=256, \n",
    "        tau=0.005, \n",
    "        gamma=0.99, \n",
    "        train_freq=1, \n",
    "        gradient_steps=1, \n",
    "        action_noise=None, \n",
    "        replay_buffer_class=None, \n",
    "        replay_buffer_kwargs=None, \n",
    "        optimize_memory_usage=False, \n",
    "        policy_delay=2, \n",
    "        target_policy_noise=0.2, \n",
    "        target_noise_clip=0.5, \n",
    "        stats_window_size=100, \n",
    "        tensorboard_log=log_dir, \n",
    "        policy_kwargs=None, \n",
    "        verbose=0, \n",
    "        seed=None, \n",
    "        device='auto', \n",
    "        _init_setup_model=True\n",
    "    )\n",
    "    # 4) Train\n",
    "    model.learn(total_timesteps=200_000, tb_log_name=\"TD3_airsim\")  # or more\n",
    "    \n",
    "    # 5) Save\n",
    "    model.save(\"TD3_airsim_model\")\n",
    "    print(\"Training complete! Model saved.\")\n",
    "    \n",
    "    # 6) Evaluate or Test\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sb3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "        # Use float32 for all arrays to match network precision\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(max_size, dtype=np.float32)  # Flattened array\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros(max_size, dtype=np.float32)    # Flattened array\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Ensure incoming data is on CPU and in numpy format\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(action):\n",
    "            action = action.detach().cpu().numpy()\n",
    "        if torch.is_tensor(reward):\n",
    "            reward = reward.detach().cpu().numpy()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(done):\n",
    "            done = done.detach().cpu().numpy()\n",
    "            \n",
    "        np.copyto(self.states[self.ptr], state)\n",
    "        np.copyto(self.actions[self.ptr], action)\n",
    "        self.rewards[self.ptr] = reward\n",
    "        np.copyto(self.next_states[self.ptr], next_state)\n",
    "        self.dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize buffers with correct shapes\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((max_size, 1), dtype=np.float32)  # Changed shape to (max_size, 1)\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size, 1), dtype=np.float32)    # Changed shape to (max_size, 1)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to numpy arrays and ensure correct shapes\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        reward = np.array(reward, dtype=np.float32).reshape(1)\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        done = np.array(done, dtype=np.float32).reshape(1)\n",
    "\n",
    "        # Store transition\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        # Update pointer and size\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=8192):\n",
    "        super().__init__()\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature_net(state)\n",
    "        return self.max_action * self.action_net(features)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=4096):\n",
    "        super().__init__()\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(act_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_features = self.state_net(state)\n",
    "        action_features = self.action_net(action)\n",
    "        features = torch.cat([state_features, action_features], dim=-1)\n",
    "        return self.q_net(features)\n",
    "    \"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        \n",
    "        # Use orthogonal initialization\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.q2_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for net in [self.q1_net, self.q2_net]:\n",
    "            for m in net.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        # Get Q-values\n",
    "        q1 = self.q1_net(sa)\n",
    "        q2 = self.q2_net(sa)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        return self.q1_net(sa)\n",
    "\"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.001):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.target_entropy = -float(actor.__init_args__[1])\n",
    "        \n",
    "        # Initialize episode tracking\n",
    "        self.current_episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        \n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it, noise):\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        reward = reward.view(-1)\n",
    "        done = done.view(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_Q1 = self.critic_target1(next_state, next_action)\n",
    "            target_Q2 = self.critic_target2(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            \n",
    "            if len(reward.shape) == 1:\n",
    "                reward = reward.unsqueeze(-1)\n",
    "            if len(done.shape) == 1:\n",
    "                done = done.unsqueeze(-1)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "        \n",
    "        # Critic 1 update\n",
    "        current_Q1 = self.critic1(state, action)\n",
    "        critic_mse_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_l2_reg1 = 0.00001 * sum(torch.sum(param ** 2) for param in self.critic1.parameters())\n",
    "        critic_loss1 = critic_mse_loss1 + critic_l2_reg1\n",
    "        \n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward(retain_graph=True) \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), 1.0)\n",
    "        self.critic_optimizer1.step()\n",
    "        \n",
    "        # Critic 2 update\n",
    "        current_Q2 = self.critic2(state, action)\n",
    "        critic_mse_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "        critic_l2_reg2 = 0 #0.00001 * sum(torch.sum(param ** 2) for param in self.critic2.parameters())\n",
    "        critic_loss2 = critic_mse_loss2 + critic_l2_reg2\n",
    "        \n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), 1.0)\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        if total_it % policy_delay == 0:\n",
    "            actor_actions = self.actor(state)\n",
    "            actor_loss = -self.critic1(state, actor_actions).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "        \n",
    "        return {\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None\n",
    "        }\n",
    "    \"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self._hard_update_targets()\n",
    "\n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it):\n",
    "        # Sample from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Ensure proper dimensions\n",
    "        reward = reward.view(-1, 1)\n",
    "        done = done.view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (torch.randn_like(action) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target1(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, _ = self.critic1(state, action)\n",
    "        current_Q2, _ = self.critic2(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critics\n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward()\n",
    "        self.critic_optimizer1.step()\n",
    "\n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if total_it % policy_delay == 0:\n",
    "            # Compute actor loss\n",
    "            actor_action = self.actor(state)\n",
    "            Q1, _ = self.critic1(state, actor_action)\n",
    "            actor_loss = -Q1.mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "\n",
    "        return {\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def compute_parameter_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_policy(actor, env, num_episodes=10, device='cuda'):\n",
    "    \"\"\"Evaluate the policy without exploration noise\"\"\"\n",
    "    eval_rewards = []\n",
    "    eval_success = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = actor(state_tensor).cpu().numpy().flatten()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if info.get('success', False):\n",
    "                eval_success += 1\n",
    "                break\n",
    "                \n",
    "        eval_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(eval_rewards),\n",
    "        'std_reward': np.std(eval_rewards),\n",
    "        'success_rate': eval_success / num_episodes,\n",
    "        'rewards': eval_rewards\n",
    "    }\n",
    "\n",
    "def run_training(env, obs_dim, act_dim, max_action, episodes=10000):\n",
    "    # Initialize environment and models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize TensorBoard writer with more descriptive name\n",
    "    current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join('logs', f'TD3_train_{current_time}')\n",
    "    train_writer = SummaryWriter(log_dir + '/train')\n",
    "    eval_writer = SummaryWriter(log_dir + '/eval')\n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "    \n",
    "    # Model initialization\n",
    "    actor = TD3Actor(obs_dim, act_dim, max_action).to(device)\n",
    "    critic1 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    critic2 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=3e-2)\n",
    "    critic_optimizer1 = optim.Adam(critic1.parameters(), lr=3e-2)\n",
    "    critic_optimizer2 = optim.Adam(critic2.parameters(), lr=3e-2)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    total_steps = 0\n",
    "    current_stage = 0\n",
    "    best_rewards = {i: float('-inf') for i in range(9)}\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load('models/TD3/Model/mtp_model.pth')\n",
    "        actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        critic1.load_state_dict(checkpoint['critic1_state_dict'])\n",
    "        critic2.load_state_dict(checkpoint['critic2_state_dict'])\n",
    "        actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        critic_optimizer1.load_state_dict(checkpoint['critic1_optimizer_state_dict'])\n",
    "        critic_optimizer2.load_state_dict(checkpoint['critic2_optimizer_state_dict'])\n",
    "        \n",
    "        try:\n",
    "            total_steps = checkpoint['total_steps']\n",
    "            current_stage = checkpoint['stage']\n",
    "            best_rewards = checkpoint['best_rewards']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        print(f\"Loaded checkpoint successfully. Total steps: {total_steps}\")\n",
    "    except:\n",
    "        print(\"No checkpoint found, starting fresh\")\n",
    "\n",
    "    # Initialize or load replay buffer\n",
    "    try:\n",
    "        replay_buffer = ReplayBuffer.load('models/TD3/Replay_Buffer/mtp_replay_buffer.pkl')\n",
    "        print(\"Loaded buffer successfully\")\n",
    "    except:\n",
    "        replay_buffer = ReplayBuffer(max_size=1_000_000, obs_dim=obs_dim, act_dim=act_dim)\n",
    "        print(\"No buffer found, starting fresh\")\n",
    "    \n",
    "    trainer = TD3Trainer(\n",
    "        actor=actor,\n",
    "        critic1=critic1,\n",
    "        critic2=critic2,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_optimizer1=critic_optimizer1,\n",
    "        critic_optimizer2=critic_optimizer2,\n",
    "        max_action=max_action,\n",
    "        device=device\n",
    "    )\n",
    "    trainer._hard_update_targets()\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 256\n",
    "    warmup_steps = 10000\n",
    "    noise_std = 0.1\n",
    "    noise_clip = 0.5\n",
    "    policy_delay = 2\n",
    "    exploration_noise = 0.2\n",
    "    eval_freq = 1000  # Evaluate every 1000 episodes\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('models/TD3/Replay_Buffer', exist_ok=True)\n",
    "    os.makedirs('models/TD3/Model', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    episode_rewards = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    stage_rewards = {i: [] for i in range(9)}\n",
    "    best_eval_reward = float('-inf')\n",
    "    \n",
    "    def get_exploration_noise(action):\n",
    "        base_noise = (torch.randn_like(action) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "        return base_noise\n",
    "    \n",
    "    def log_training_step(train_info, step):\n",
    "        # Log losses\n",
    "        train_writer.add_scalar('Loss/Critic1', train_info['critic_loss1'], step)\n",
    "        train_writer.add_scalar('Loss/Critic2', train_info['critic_loss2'], step)\n",
    "        if train_info['actor_loss'] is not None:\n",
    "            train_writer.add_scalar('Loss/Actor', train_info['actor_loss'], step)\n",
    "        \n",
    "        # Log gradient norms\n",
    "        train_writer.add_scalar('Gradients/Actor_Norm', compute_gradient_norm(actor), step)\n",
    "        train_writer.add_scalar('Gradients/Critic1_Norm', compute_gradient_norm(critic1), step)\n",
    "        train_writer.add_scalar('Gradients/Critic2_Norm', compute_gradient_norm(critic2), step)\n",
    "        \n",
    "        # Log parameter norms\n",
    "        train_writer.add_scalar('Parameters/Actor_Norm', compute_parameter_norm(actor), step)\n",
    "        train_writer.add_scalar('Parameters/Critic1_Norm', compute_parameter_norm(critic1), step)\n",
    "        train_writer.add_scalar('Parameters/Critic2_Norm', compute_parameter_norm(critic2), step)\n",
    "        \n",
    "        # Log learning rates\n",
    "        train_writer.add_scalar('LearningRate/Actor', actor_optimizer.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic1', critic_optimizer1.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic2', critic_optimizer2.param_groups[0]['lr'], step)\n",
    "    \n",
    "    def run_evaluation(episode):\n",
    "        # Create a separate environment for evaluation\n",
    "        eval_env = env.__class__()  # Assuming env has a constructor that takes no arguments\n",
    "        eval_env.training_stage = env.training_stage  # Sync the training stage\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_results = evaluate_policy(actor, eval_env, num_episodes=10, device=device)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        eval_writer.add_scalar('Eval/Mean_Reward', eval_results['mean_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Reward_Std', eval_results['std_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Success_Rate', eval_results['success_rate'], episode)\n",
    "        \n",
    "        # Log reward distribution\n",
    "        eval_writer.add_histogram('Eval/Reward_Distribution', \n",
    "                                torch.tensor(eval_results['rewards']), \n",
    "                                episode)\n",
    "        \n",
    "        eval_env.close()\n",
    "        return eval_results\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(range(episodes), desc=\"Training\")\n",
    "    for episode in progress_bar:\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        episode_steps = 0\n",
    "        current_stage = env.training_stage\n",
    "        \n",
    "        # Log current stage\n",
    "        train_writer.add_scalar('Training/Current_Stage', current_stage, total_steps)\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            if total_steps < warmup_steps:\n",
    "                action = np.random.uniform(-max_action, max_action, size=act_dim)\n",
    "                train_writer.add_scalar('Training/Exploration_Type', 0, total_steps)  # 0 for random\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                    action = actor(state_tensor).cpu().numpy().flatten()\n",
    "                    current_noise = get_exploration_noise(action)\n",
    "                    action += np.random.normal(0, current_noise, size=act_dim)\n",
    "                action = np.clip(action, -max_action, max_action)\n",
    "                train_writer.add_scalar('Training/Exploration_Type', 1, total_steps)  # 1 for policy\n",
    "                train_writer.add_scalar('Training/Exploration_Noise', current_noise, total_steps)\n",
    "            \n",
    "            # Log action statistics\n",
    "            train_writer.add_histogram('Actions/Distribution', action, total_steps)\n",
    "            train_writer.add_scalar('Actions/Mean', np.mean(action), total_steps)\n",
    "            train_writer.add_scalar('Actions/Std', np.std(action), total_steps)\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Update state and metrics\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Train agent\n",
    "            if total_steps > warmup_steps and len(replay_buffer.states) > batch_size:\n",
    "                train_info = trainer.train_step(\n",
    "                    replay_buffer=replay_buffer,\n",
    "                    batch_size=batch_size,\n",
    "                    noise_std=noise_std,\n",
    "                    noise_clip=noise_clip,\n",
    "                    policy_delay=policy_delay,\n",
    "                    total_it=total_steps,\n",
    "                    action = action,\n",
    "                    reward = reward,\n",
    "                    noise = current_noise\n",
    "                )\n",
    "                \n",
    "                # Log training metrics\n",
    "                log_training_step(train_info, total_steps)\n",
    "        \n",
    "        # Episode completion logging\n",
    "        recent_rewards.append(episode_reward)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        stage_rewards[current_stage].append(episode_reward)\n",
    "        \n",
    "        # Log episode metrics\n",
    "        train_writer.add_scalar('Episode/Reward', episode_reward, episode)\n",
    "        train_writer.add_scalar('Episode/Steps', episode_steps, episode)\n",
    "        train_writer.add_scalar('Episode/Average_100_Episodes', np.mean(recent_rewards), episode)\n",
    "        train_writer.add_scalar('Training/Success_Count', env.success_count, episode)\n",
    "        train_writer.add_scalar('Training/Buffer_Size', len(replay_buffer), episode)\n",
    "        \n",
    "        # Run evaluation periodically\n",
    "        if episode % eval_freq == 0:\n",
    "            eval_results = run_evaluation(episode)\n",
    "            \n",
    "            # Save best model based on evaluation\n",
    "            if eval_results['mean_reward'] > best_eval_reward:\n",
    "                best_eval_reward = eval_results['mean_reward']\n",
    "                torch.save({\n",
    "                    'actor_state_dict': actor.state_dict(),\n",
    "                    'critic1_state_dict': critic1.state_dict(),\n",
    "                    'critic2_state_dict': critic2.state_dict(),\n",
    "                    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                    'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                    'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                    'total_steps': total_steps,\n",
    "                    'episode': episode,\n",
    "                    'stage': current_stage,\n",
    "                    'eval_reward': best_eval_reward,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, 'models/TD3/Model/mtp_model_best_eval.pth')\n",
    "        \n",
    "        # Stage-specific metrics\n",
    "        if len(stage_rewards[current_stage]) >= 100:\n",
    "            current_avg_reward = np.mean(stage_rewards[current_stage][-100:])\n",
    "            train_writer.add_scalar(f'Stage_{current_stage}/Average_100_Episodes', \n",
    "                                  current_avg_reward, \n",
    "                                  len(stage_rewards[current_stage]))\n",
    "            \n",
    "            if current_avg_reward > best_rewards[current_stage]:\n",
    "                best_rewards[current_stage] = current_avg_reward\n",
    "                train_writer.add_scalar(f'Stage_{current_stage}/Best_Average_Reward', \n",
    "                                      current_avg_reward, \n",
    "                                      len(stage_rewards[current_stage]))\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'stage': current_stage,\n",
    "            'reward': f'{episode_reward:.2f}',\n",
    "            'avg_reward': f'{np.mean(recent_rewards):.2f}' if episode_rewards else 'N/A',\n",
    "            'success_count': env.success_count\n",
    "        })\n",
    "        \n",
    "        # Check for training completion\n",
    "        if current_stage == 8 and env.success_count >= env.stage_threshold:\n",
    "            print(\"\\n=== Training completed successfully! ===\")\n",
    "            env.close()\n",
    "            print(\"Environment closed. Running final evaluation...\")\n",
    "            final_eval_results = run_evaluation(episode)\n",
    "            print(f\"Final evaluation results: {final_eval_results}\")\n",
    "            train_writer.close()\n",
    "            eval_writer.close()\n",
    "            break\n",
    "        \n",
    "        # Periodic checkpointing\n",
    "        if episode % 2500 == 0:\n",
    "            checkpoint_path = f'models/TD3/Model/mtp_backup_{episode}.pth'\n",
    "            torch.save({\n",
    "                'actor_state_dict': actor.state_dict(),\n",
    "                'critic1_state_dict': critic1.state_dict(),\n",
    "                'critic2_state_dict': critic2.state_dict(),\n",
    "                'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                'episode': episode,\n",
    "                'total_steps': total_steps,\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    return actor, critic1, critic2, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n",
      "Using device: cuda\n",
      "TensorBoard logs will be saved to: logs\\TD3_train_20250117-221922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henhe\\AppData\\Local\\Temp\\ipykernel_64508\\2133005428.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('models/TD3/Model/mtp_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting fresh\n",
      "No buffer found, starting fresh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/10000 [00:01<5:29:06,  1.97s/it, stage=0, reward=-10.00, avg_reward=-10.00, success_count=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 1000/10000 [40:35<11:15:19,  4.50s/it, stage=0, reward=-10.42, avg_reward=-9.95, success_count=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2000/10000 [1:39:48<2:36:54,  1.18s/it, stage=0, reward=-10.45, avg_reward=-9.80, success_count=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 2501/10000 [2:07:21<8:31:45,  4.09s/it, stage=0, reward=-10.53, avg_reward=-9.73, success_count=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_2500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3000/10000 [2:31:58<2:40:38,  1.38s/it, stage=0, reward=-10.66, avg_reward=-9.99, success_count=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4000/10000 [3:27:26<2:00:32,  1.21s/it, stage=0, reward=-10.49, avg_reward=-9.90, success_count=0]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5000/10000 [4:17:23<3:29:26,  2.51s/it, stage=0, reward=-10.00, avg_reward=-10.03, success_count=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5001/10000 [4:17:27<3:58:06,  2.86s/it, stage=0, reward=-10.39, avg_reward=-10.04, success_count=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_5000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 6000/10000 [5:08:54<4:10:34,  3.76s/it, stage=0, reward=-8.34, avg_reward=-9.91, success_count=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7000/10000 [5:59:49<2:26:55,  2.94s/it, stage=0, reward=-10.40, avg_reward=-10.10, success_count=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 7501/10000 [6:25:15<1:31:42,  2.20s/it, stage=0, reward=-10.41, avg_reward=-9.93, success_count=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_7500.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8000/10000 [6:52:08<33:13,  1.00it/s, stage=0, reward=-10.40, avg_reward=-9.89, success_count=0]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9000/10000 [7:42:51<1:06:58,  4.02s/it, stage=0, reward=-10.00, avg_reward=-9.98, success_count=0] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [8:32:50<00:00,  3.08s/it, stage=0, reward=-10.00, avg_reward=-9.90, success_count=0]  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m actor, critic1, critic2, rewards \u001b[38;5;241m=\u001b[39m run_training(\n\u001b[0;32m     10\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m     11\u001b[0m     obs_dim\u001b[38;5;241m=\u001b[39mobs_dim,\n\u001b[0;32m     12\u001b[0m     act_dim\u001b[38;5;241m=\u001b[39mact_dim,\n\u001b[0;32m     13\u001b[0m     max_action\u001b[38;5;241m=\u001b[39mmax_action,\n\u001b[0;32m     14\u001b[0m     episodes\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10_000\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "env = CosysAirSimEnv_Velocity(training_stage=0)\n",
    "\n",
    "# Get environment dimensions\n",
    "obs_dim = env.observation_space.shape[0]  \n",
    "act_dim = env.action_space.shape[0]     \n",
    "max_action = float(env.action_space.high[0]) \n",
    "\n",
    "# Run training\n",
    "actor, critic1, critic2, rewards = run_training(\n",
    "    env=env,\n",
    "    obs_dim=obs_dim,\n",
    "    act_dim=act_dim,\n",
    "    max_action=max_action,\n",
    "    episodes= 10_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
