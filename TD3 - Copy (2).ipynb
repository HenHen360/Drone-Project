{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n",
      "Connected to AirSim!\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Main libraries\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import cosysairsim as airsim\n",
    "from cosysairsim import MultirotorClient\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#General\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from typing import Dict, Any, Tuple\n",
    "import logging\n",
    "from logging import handlers\n",
    "from copy import deepcopy\n",
    "from itertools import count\n",
    "import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn.utils as torch_utils\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "client = airsim.MultirotorClient()\n",
    "client.confirmConnection()\n",
    "print(\"Connected to AirSim!\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(device)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\naction=[0,0,1,0]\\nmax_angles = np.radians([30, 30, 45])  # Reduced pitch/roll angles\\npitch = np.clip(action[0] * max_angles[0], -max_angles[0], max_angles[0])\\nroll = np.clip(action[1] * max_angles[1], -max_angles[1], max_angles[1])\\n\\n# Use exponential mapping for throttle to give more control in hover region\\nthrottle = (action[2])+1 / 2  # Centered around 0.5\\nyaw_rate = np.clip(action[3] * max_angles[2], -max_angles[2], max_angles[2])\\n\\n# Add small time delay for stability\\nclient.moveByRollPitchYawrateThrottleAsync(\\n    roll=roll,\\n    pitch=pitch,\\n    yaw_rate=yaw_rate,\\n    throttle=throttle,\\n    duration=1000  # Reduced duration for more frequent updates\\n).join()\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MultirotorClient()\n",
    "client.confirmConnection()\n",
    "client.enableApiControl(True)\n",
    "client.armDisarm(True)\n",
    "client.takeoffAsync().join\n",
    "\"\"\"\n",
    "action=[0,0,1,0]\n",
    "max_angles = np.radians([30, 30, 45])  # Reduced pitch/roll angles\n",
    "pitch = np.clip(action[0] * max_angles[0], -max_angles[0], max_angles[0])\n",
    "roll = np.clip(action[1] * max_angles[1], -max_angles[1], max_angles[1])\n",
    "\n",
    "# Use exponential mapping for throttle to give more control in hover region\n",
    "throttle = (action[2])+1 / 2  # Centered around 0.5\n",
    "yaw_rate = np.clip(action[3] * max_angles[2], -max_angles[2], max_angles[2])\n",
    "\n",
    "# Add small time delay for stability\n",
    "client.moveByRollPitchYawrateThrottleAsync(\n",
    "    roll=roll,\n",
    "    pitch=pitch,\n",
    "    yaw_rate=yaw_rate,\n",
    "    throttle=throttle,\n",
    "    duration=1000  # Reduced duration for more frequent updates\n",
    ").join()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_quaternion(self, roll=0.0, pitch=0.0, yaw=0.0):\n",
    "    \"\"\"\n",
    "    Convert roll, pitch, yaw (in radians) to airsim.Quaternionr (x, y, z, w).\n",
    "    AirSim uses a right-handed coordinate system with X=East, Y=North, Z=Down.\n",
    "    \"\"\"\n",
    "    cy = math.cos(yaw * 0.5)\n",
    "    sy = math.sin(yaw * 0.5)\n",
    "    cr = math.cos(roll * 0.5)\n",
    "    sr = math.sin(roll * 0.5)\n",
    "    cp = math.cos(pitch * 0.5)\n",
    "    sp = math.sin(pitch * 0.5)\n",
    "\n",
    "    w = cy * cr * cp + sy * sr * sp\n",
    "    x = cy * sr * cp - sy * cr * sp\n",
    "    y = cy * cr * sp + sy * sr * cp\n",
    "    z = sy * cr * cp - cy * sr * sp\n",
    "\n",
    "    return airsim.Quaternionr(x_val=x, y_val=y, z_val=z, w_val=w)\n",
    "\n",
    "\n",
    "spawn_x = random.uniform(-5, 5)\n",
    "spawn_y = random.uniform(-5, 5)\n",
    "spawn_z = random.uniform(-1,1)\n",
    "spawn_yaw = random.uniform(-math.pi, math.pi)\n",
    "\n",
    "q = to_quaternion(0, 0, spawn_yaw)\n",
    "pose = airsim.Pose(\n",
    "    airsim.Vector3r(spawn_x, spawn_y, spawn_z), q)\n",
    "\n",
    "client.simSetVehiclePose(pose, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.moveToPositionAsync(0,0,-10, 0.5).join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vector3r> {   'x_val': 0.0,\n",
      "    'y_val': 0.0,\n",
      "    'z_val': 2.055117607116699}\n"
     ]
    }
   ],
   "source": [
    "state = client.getMultirotorState()\n",
    "kinematics = state.kinematics_estimated\n",
    "pos = kinematics.position\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosysAirSimEnv_Basic(gym.Env):\n",
    "    def __init__(self, training_stage=0):\n",
    "        super(CosysAirSimEnv_Basic, self).__init__()\n",
    "\n",
    "        self.client = MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(21,), dtype=np.float32)\n",
    "\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.episode = 0\n",
    "        self.wind_factor = 0.0\n",
    "        self.last_throttle = 0.6\n",
    "        self.success_count = 0\n",
    "        \n",
    "        self.success_threshold = 10\n",
    "        \n",
    "        self.training_stage = training_stage\n",
    "        self.setup_stage_params()\n",
    "        self.stage_progress = 0\n",
    "        self.stage_threshold = 25\n",
    "        \n",
    "        self.hover_survival_count = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.current_target = self._generate_new_target()\n",
    "        \n",
    "        self.min_distance = float('inf')\n",
    "        high = np.array([\n",
    "            1.0, 1.0, 1.0,      # normalized position\n",
    "            1.0, 1.0, 1.0,      # normalized velocity\n",
    "            1.0, 1.0, 1.0,      # normalized angular velocity\n",
    "            3.0, 3.0, 3.0, \n",
    "            1.0, 1.0, 1.0,      # normalized acceleration (in G's)\n",
    "            1.0, 1.0, 1.0,      # normalized target position\n",
    "            1.0,                # normalized distance\n",
    "            1.0,                 # normalized height error\n",
    "            1.0\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "    def setup_stage_params(self):\n",
    "        self.params = {\n",
    "            0: {  # Hovering stage\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 10.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'hover',\n",
    "                'target_radius': 0.5\n",
    "            },\n",
    "            1: {  # Near vertical movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 10.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'vertical',\n",
    "                'target_radius': 0.5\n",
    "            },\n",
    "            2: {  # Close-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 5.0\n",
    "            },\n",
    "            3: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 400,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 10.0\n",
    "            },\n",
    "            4: {  # Long-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0\n",
    "            },\n",
    "            5: {  # Extended range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [30.0, 30.0, 15.0],  # Expanded area\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 25.0  # Increased maximum distance\n",
    "            },\n",
    "            6: {  # Recovery training\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 500,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.3,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 50  # Apply random action every 50 steps\n",
    "            },\n",
    "            7: {  # Moderate wind\n",
    "                'wind': 0.5,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,\n",
    "                'max_target_dist': 25.0\n",
    "            },\n",
    "            8: {  # Strong wind\n",
    "                'wind': 1.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 600,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.5,  # Slightly larger radius due to strong wind\n",
    "                'max_target_dist': 25.0\n",
    "            }\n",
    "        }\n",
    "        self.current_params = self.params[self.training_stage]\n",
    "        \n",
    "    def _generate_new_target(self):\n",
    "        \"\"\"Internal method to generate a new target based on current parameters\"\"\"\n",
    "        params = self.current_params\n",
    "        \n",
    "        if params['target_type'] == 'hover':\n",
    "            return np.array([0.0, 0.0, -1.0])\n",
    "            \n",
    "        elif params['target_type'] == 'vertical':\n",
    "            height = np.random.uniform(*params['height_range'])\n",
    "            return np.array([0.0, 0.0, height])\n",
    "            \n",
    "        elif params['target_type'] == 'horizontal_near':\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            dist = np.random.uniform(2.0, params['max_target_dist'])\n",
    "            return np.array([\n",
    "                dist * np.cos(angle),\n",
    "                dist * np.sin(angle),\n",
    "                -2.0\n",
    "            ])\n",
    "            \n",
    "        elif params['target_type'] in ['free_near', 'free']:\n",
    "            while True:\n",
    "                point = np.random.uniform(\n",
    "                    low=[-params['max_target_dist'], -params['max_target_dist'], -4.0],\n",
    "                    high=[params['max_target_dist'], params['max_target_dist'], -1.0],\n",
    "                    size=(3,)\n",
    "                )\n",
    "                if np.linalg.norm(point - self._get_current_position()) > 2.0:\n",
    "                    return point\n",
    "\n",
    "    def _get_target(self):\n",
    "        \"\"\"Return the current target\"\"\"\n",
    "        return self.current_target\n",
    "    \n",
    "    def _get_distance_target(self):\n",
    "        return np.linalg.norm(self._get_current_position() - self._get_target())\n",
    "        \n",
    "    def _get_current_position(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        pos = kinematics.position\n",
    "\n",
    "        return np.array([pos.x_val, pos.y_val, pos.z_val], dtype=np.float32)\n",
    "\n",
    "    def _get_current_velocity(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        vel = kinematics.linear_velocity\n",
    "\n",
    "        return np.array([vel.x_val, vel.y_val, vel.z_val], dtype=np.float32)\n",
    "    \n",
    "    def _get_imu_data(self):\n",
    "        imu_data = self.client.getImuData()\n",
    "        # IMU typically gives angular velocity and linear acceleration\n",
    "        # We'll store them as [wx, wy, wz, ax, ay, az]\n",
    "        ang_vel = imu_data.angular_velocity\n",
    "        lin_acc = imu_data.linear_acceleration\n",
    "        return np.array([\n",
    "            ang_vel.x_val, ang_vel.y_val, ang_vel.z_val,\n",
    "            lin_acc.x_val, lin_acc.y_val, lin_acc.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_mag_data(self):\n",
    "        mag_data = self.client.getMagnetometerData()\n",
    "        # magnetometer.x, .y, .z\n",
    "        return np.array([\n",
    "            mag_data.magnetic_field_body.x_val,\n",
    "            mag_data.magnetic_field_body.y_val,\n",
    "            mag_data.magnetic_field_body.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_gps_data(self):\n",
    "        gps_data = self.client.getGpsData()\n",
    "        # gps_data.gnss.geo_point.latitude, .longitude, .altitude\n",
    "        return np.array([\n",
    "            gps_data.gnss.geo_point.latitude,\n",
    "            gps_data.gnss.geo_point.longitude,\n",
    "            gps_data.gnss.geo_point.altitude\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_baro_data(self):\n",
    "        baro_data = self.client.getBarometerData()\n",
    "        # baro_data.altitude, baro_data.pressure, baro_data.qnh\n",
    "        # We'll just use altitude\n",
    "        return np.array([baro_data.altitude], dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        #Get and standardize observations.\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # We'll ignore GPS and magnetometer data as they're less relevant for this task\n",
    "        # and could add noise to the learning process\n",
    "\n",
    "        # Standardize position: Scale by area bounds\n",
    "        normalized_pos = position / np.array(self.current_params['area'])\n",
    "\n",
    "        # Standardize velocity: Most drones operate within -10 to 10 m/s range\n",
    "        normalized_vel = np.clip(velocity / 10.0, -1.0, 1.0)\n",
    "\n",
    "        # Standardize IMU data\n",
    "        angular_vel = imu_data[:3] / 10.0  # Angular velocity (rad/s)\n",
    "        linear_acc = imu_data[3:6] / 9.81  # Linear acceleration (normalize by g)\n",
    "\n",
    "        # Standardize target and distance\n",
    "        normalized_target = target / np.array(self.current_params['area'])\n",
    "        normalized_distance = distance / np.linalg.norm(self.current_params['area'])\n",
    "\n",
    "        # Height error (important for hovering)\n",
    "        height_error = (position[2] - target[2]) / self.current_params['area'][2]\n",
    "\n",
    "        # Combine all observations\n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]   - Normalized position (x, y, z)\n",
    "            normalized_vel,        # [3:6]   - Normalized velocity (vx, vy, vz)\n",
    "            angular_vel,          # [6:9]   - Normalized angular velocity (wx, wy, wz)\n",
    "            linear_acc,          # [9:12]  - Normalized linear acceleration (ax, ay, az)\n",
    "            normalized_target,    # [12:15] - Normalized target position\n",
    "            [normalized_distance],# [15]    - Normalized distance to target\n",
    "            [height_error]       # [16]    - Normalized height error\n",
    "        ])\n",
    "        # Clip all values to ensure they stay within a reasonable range\n",
    "        observation = np.clip(observation, -5.0, 5.0)\n",
    "        \n",
    "        return observation.astype(np.float32)\"\"\"\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # Normalize position relative to the current stage's boundaries\n",
    "        area_bounds = np.array(self.current_params['area'])\n",
    "        normalized_pos = np.clip(position / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize velocity (typical drone speeds rarely exceed ±10 m/s)\n",
    "        velocity_scale = np.array([10.0, 10.0, 5.0])  # Less vertical velocity range\n",
    "        normalized_vel = np.clip(velocity / velocity_scale, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize IMU data\n",
    "        # Angular velocity (typically ±5 rad/s)\n",
    "        angular_vel = np.clip(imu_data[:3] / 5.0, -1.0, 1.0)\n",
    "        # Linear acceleration (normalize by g ≈ 9.81 m/s²)\n",
    "        linear_acc = np.clip(imu_data[3:6] / 9.81, -3.0, 3.0)  # Allow for higher G-forces\n",
    "        \n",
    "        # Normalize target position\n",
    "        normalized_target = np.clip(target / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize distance relative to maximum possible distance in current area\n",
    "        max_possible_distance = np.linalg.norm(area_bounds)\n",
    "        normalized_distance = np.clip(distance / max_possible_distance, 0.0, 1.0)\n",
    "        \n",
    "        # Height error normalization\n",
    "        max_height_error = area_bounds[2]\n",
    "        height_error = (position[2] - target[2]) / max_height_error\n",
    "        normalized_height_error = np.clip(height_error, -1.0, 1.0)\n",
    "\n",
    "        relative_pos = (target - position) / np.array(self.current_params['area'])\n",
    "        \n",
    "        target_direction = (target - position) / (np.linalg.norm(target - position) + 1e-6)\n",
    "        relative_vel = np.dot(velocity, target_direction)\n",
    "        \n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]\n",
    "            normalized_vel,        # [3:6]\n",
    "            angular_vel,          # [6:9]\n",
    "            linear_acc,          # [9:12]\n",
    "            relative_pos,\n",
    "            normalized_target,        # [12:15]\n",
    "            [normalized_distance],# [15]\n",
    "            [normalized_height_error],      # [16]\n",
    "            [relative_vel]       # [17] - Added velocity towards target\n",
    "        ])\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _apply_wind(self):\n",
    "        \"\"\"Applies a simulated wind force to the drone.\"\"\"\n",
    "        wind_x = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_y = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_z = np.random.uniform(-self.wind_factor / 2, self.wind_factor / 2)\n",
    "        self.client.simSetWind(airsim.Vector3r(wind_x, wind_y, wind_z))\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # Reset the drone\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        self.client.takeoffAsync().join()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.episode += 1\n",
    "        self._apply_wind()\n",
    "        \n",
    "        if not hasattr(self, 'current_target') or self.current_target is None:\n",
    "            self._get_target()  # This will generate a new target\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        if self.training_stage == 6:\n",
    "            if self.current_step % self.current_params['recovery_interval'] == 0:\n",
    "                # Apply random action\n",
    "                random_action = np.random.uniform(-1, 1, size=4)\n",
    "                self.recovery_action_timer = 10  # Recovery period\n",
    "                action = random_action\n",
    "            elif self.recovery_action_timer > 0:\n",
    "                self.recovery_action_timer -= 1\n",
    "        # Process controls - separate throttle handling\n",
    "        max_angles = np.radians([45, 45, 45])  # pitch, roll, yaw_rate\n",
    "        \n",
    "        pitch = np.clip(action[0] * max_angles[0], -max_angles[0], max_angles[0])\n",
    "        roll = np.clip(action[1] * max_angles[1], -max_angles[1], max_angles[1])\n",
    "        throttle = (action[2] + 1.0) / 2.0  # Convert from [-1,1] to [0,1]\n",
    "        yaw_rate = np.clip(action[3] * max_angles[2], -max_angles[2], max_angles[2])\n",
    "        \n",
    "        # Execute action in AirSim\n",
    "        self.client.moveByRollPitchYawrateThrottleAsync(\n",
    "            roll=roll,\n",
    "            pitch=pitch,\n",
    "            yaw_rate=yaw_rate,\n",
    "            throttle=throttle,\n",
    "            duration=0.25\n",
    "        ).join()\n",
    "        \n",
    "        self._apply_wind()\n",
    "        observation = self._get_observation()\n",
    "        reward, terminated = self._calculate_reward()\n",
    "        \n",
    "        truncated = self.current_step >= self.current_params['max_steps']\n",
    "        #if truncated:\n",
    "        #    reward += 10\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:\n",
    "            # If we ended the episode by hitting max_steps (truncate) WITHOUT crashing (terminated=False),\n",
    "            # then we survived. Increment the counter.\n",
    "            if truncated:\n",
    "                self.hover_survival_count += 1\n",
    "                reward += 50\n",
    "\n",
    "                # If we've survived enough times in a row, move to stage 1\n",
    "                if self.hover_survival_count >= self.num_consecutive_survivals_needed:\n",
    "                    self.training_stage = 1\n",
    "                    self.setup_stage_params()\n",
    "                    self.hover_survival_count = 0\n",
    "                    print(\n",
    "                        f\"=== Moved to stage {self.training_stage} after \"\n",
    "                        f\"{self.num_consecutive_survivals_needed} consecutive survival episodes. ===\"\n",
    "                    )\n",
    "        \"\"\"\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "    def _calculate_reward(self):\n",
    "        pos = self._get_current_position()\n",
    "        vel = self._get_current_velocity()\n",
    "        angular_vel = self._get_imu_data()[:3]\n",
    "        target = self._get_target()\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        reward_scale = 0.005\n",
    "        reward = 0.0\n",
    "        # Immediate failure conditions (keep these)\n",
    "        if self.client.simGetCollisionInfo().has_collided:\n",
    "            reward -= 200*reward_scale\n",
    "            return reward, True\n",
    "        if any(abs(p) > a for p, a in zip(pos, self.current_params['area'])):\n",
    "            reward -= 100*reward_scale\n",
    "            return reward, True\n",
    "        # Main distance reward - smoother gradient\n",
    "        distance_reward = -distance+1  # Linear scaling for more consistent gradient\n",
    "        \n",
    "        \"\"\"\n",
    "        # Stability penalty - much gentler\n",
    "        stability_penalty = (\n",
    "            0.05 * np.linalg.norm(angular_vel) +  # Reduced rotation penalty\n",
    "            0.02 * np.linalg.norm(vel)            # Reduced velocity penalty\n",
    "        )\n",
    "        reward -= stability_penalty\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:  # Hovering\n",
    "            # Pure hovering - focus on stability\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "            \n",
    "        elif self.training_stage == 1:  # Vertical movement\n",
    "            # Reward vertical progress toward target\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            # Penalize horizontal drift more strongly\n",
    "            horizontal_drift = np.linalg.norm(pos[:2] - target[:2])\n",
    "            reward -= (0.5 * horizontal_drift)*reward_scale\n",
    "            \n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "        elif self.training_stage == 2:  # Close-range horizontal\n",
    "            # Reward horizontal progress\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (3.0 * progress)*reward_scale  # Stronger reward for deliberate movement\n",
    "            self._prev_distance = distance\n",
    "            \n",
    "            # Height stability becomes secondary but still important\n",
    "            reward += (distance_reward)*reward_scale\n",
    "            \n",
    "        elif self.training_stage >= 3:  # Medium-range movement\n",
    "            # Balance distance and control\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (2.0 * progress)*reward_scale\n",
    "            self._prev_distance = distance\n",
    "            reward += (distance_reward)*reward_scale\n",
    "        # Success condition\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance < success_radius:\n",
    "            bonus = 0\n",
    "            bonus += (25.0)*reward_scale\n",
    "            self.consecutive_success += 1\n",
    "            \n",
    "            if self.consecutive_success % self.success_threshold == 0:\n",
    "                self.current_target = self._generate_new_target()\n",
    "                bonus += 50.0*reward_scale  \n",
    "                self.success_count += 1\n",
    "        \n",
    "            if self.success_count >= self.stage_threshold:\n",
    "                self.training_stage = min(8, self.training_stage + 1)\n",
    "                print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "                self.success_count = 0\n",
    "                self.setup_stage_params()\n",
    "                \n",
    "            return reward, True\n",
    "        return reward, False\n",
    "\n",
    "    def close(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "        self.client.armDisarm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosysAirSimEnv_Velocity(gym.Env):\n",
    "    def __init__(self, training_stage=5):\n",
    "        super(CosysAirSimEnv_Velocity, self).__init__()\n",
    "\n",
    "        self.client = MultirotorClient()\n",
    "        self.client.confirmConnection()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=-5.0, high=5.0, shape=(24,), dtype=np.float32)\n",
    "\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        self.episode = 0\n",
    "        self.wind_factor = 0.0\n",
    "        self.last_throttle = 0.6\n",
    "        self.success_count = 0\n",
    "\n",
    "        self.success_threshold = 20\n",
    "        \n",
    "        self.training_stage = training_stage\n",
    "        self.setup_stage_params()\n",
    "        self.stage_progress = 0\n",
    "        self.stage_threshold = 50\n",
    "        \n",
    "        self.hover_survival_count = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.current_target = self._generate_new_target()\n",
    "        \n",
    "        self.episode_distances = []\n",
    "        self.recovery_action_timer = 0\n",
    "        \n",
    "        high = np.array([\n",
    "            1.0, 1.0, 1.0,      # normalized position\n",
    "            1.0, 1.0, 1.0,      # normalized velocity\n",
    "            1.0, 1.0, 1.0,      # normalized angular velocity\n",
    "            3.0, 3.0, 3.0, \n",
    "            1.0, 1.0, 1.0,      # normalized acceleration (in G's)\n",
    "            1.0, 1.0, 1.0,      # normalized target position\n",
    "            1.0,                # normalized distance\n",
    "            1.0,                 # normalized height error\n",
    "            1.0,\n",
    "            1.0,\n",
    "            1.0, 1.0\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "        self.episode_min_distance = 1\n",
    "        \n",
    "    def setup_stage_params(self):\n",
    "        self.params = {\n",
    "            0: {  # Hovering stage\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 10.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'hover',\n",
    "                'target_radius': 0.2,\n",
    "                'spawn_range': 1.0\n",
    "            },\n",
    "            1: {  # Near vertical movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 10.0],\n",
    "                'max_steps': 100,\n",
    "                'target_type': 'vertical',\n",
    "                'target_radius': 0.2,\n",
    "                'height_range': [-6.0, -1.0],\n",
    "                'spawn_range': 2.0\n",
    "            },\n",
    "            2: {  # Short-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 150,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.2,\n",
    "                'max_target_dist': 2.0,\n",
    "                'spawn_range': 2.0\n",
    "            },\n",
    "            3: {  # Long-range horizontal movement\n",
    "                'wind': 0.0,\n",
    "                'area': [10.0, 10.0, 5.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'horizontal_near',\n",
    "                'target_radius': 0.2,\n",
    "                'max_target_dist': 6.0,\n",
    "                'spawn_range': 2.0\n",
    "            },\n",
    "            4: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 200,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.2,\n",
    "                'max_target_dist': 6.0,\n",
    "                'spawn_range': 3.0\n",
    "            },\n",
    "            5: {  # Medium-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [15.0, 15.0, 8.0],\n",
    "                'max_steps': 250,\n",
    "                'target_type': 'free_near',\n",
    "                'target_radius': 0.25,\n",
    "                'max_target_dist': 8.0,\n",
    "                'spawn_range': 3.0\n",
    "            },\n",
    "            6: {  # Long-range movement\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.25,\n",
    "                'max_target_dist': 15.0,\n",
    "                'spawn_range': 3.0\n",
    "            },\n",
    "            7: {  # Recovery training\n",
    "                'wind': 0.0,\n",
    "                'area': [20.0, 20.0, 10.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.25,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 50, # Apply random action every 50 steps\n",
    "                'spawn_range': 4.0  \n",
    "            },\n",
    "            8: {  # Moderate wind\n",
    "                'wind': 1.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.25,\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 150,\n",
    "                'spawn_range': 4.0\n",
    "            },\n",
    "            9: {  # Strong wind\n",
    "                'wind': 5.0,\n",
    "                'area': [30.0, 30.0, 15.0],\n",
    "                'max_steps': 300,\n",
    "                'target_type': 'free',\n",
    "                'target_radius': 0.25,  # Slightly larger radius due to strong wind\n",
    "                'max_target_dist': 15.0,\n",
    "                'recovery_interval': 150,\n",
    "                'spawn_range': 4.0\n",
    "            }\n",
    "        }\n",
    "        self.current_params = self.params[self.training_stage]\n",
    "        \n",
    "    def _generate_new_target(self):\n",
    "        \"\"\"Internal method to generate a new target based on current parameters\"\"\"\n",
    "        params = self.current_params\n",
    "        \n",
    "        if params['target_type'] == 'hover':\n",
    "            return np.array([0.0, 0.0, -1.75])\n",
    "            \n",
    "        elif params['target_type'] == 'vertical':\n",
    "            height = np.random.uniform(*params['height_range'])\n",
    "            return np.array([0.0, 0.0, height])\n",
    "            \n",
    "        elif params['target_type'] == 'horizontal_near':\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            dist = np.random.uniform(4.0, params['max_target_dist'])\n",
    "            z_val = np.random.uniform(-1.0, -2.0)\n",
    "            return np.array([\n",
    "                dist * np.cos(angle),\n",
    "                dist * np.sin(angle),\n",
    "                z_val\n",
    "            ])\n",
    "            \n",
    "        elif params['target_type'] in ['free_near', 'free']:\n",
    "            while True:\n",
    "                point = np.random.uniform(\n",
    "                    low=[-params['max_target_dist'], -params['max_target_dist'], -5.0],\n",
    "                    high=[params['max_target_dist'], params['max_target_dist'], -1.0],\n",
    "                    size=(3,)\n",
    "                )\n",
    "                if np.linalg.norm(point - self._get_current_position()) > 2.0:\n",
    "                    return point\n",
    "\n",
    "    def _get_target(self):\n",
    "        \"\"\"Return the current target\"\"\"\n",
    "        return self.current_target\n",
    "    \n",
    "    def _get_distance_target(self):\n",
    "        return np.linalg.norm(self._get_current_position() - self._get_target())\n",
    "        \n",
    "    def _get_current_position(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        pos = kinematics.position\n",
    "\n",
    "        return np.array([pos.x_val, pos.y_val, pos.z_val], dtype=np.float32)\n",
    "\n",
    "    def _get_current_velocity(self):\n",
    "        state = self.client.getMultirotorState()\n",
    "        kinematics = state.kinematics_estimated\n",
    "        vel = kinematics.linear_velocity\n",
    "\n",
    "        return np.array([vel.x_val, vel.y_val, vel.z_val], dtype=np.float32)\n",
    "    \n",
    "    def _get_imu_data(self):\n",
    "        imu_data = self.client.getImuData()\n",
    "        # IMU typically gives angular velocity and linear acceleration\n",
    "        # We'll store them as [wx, wy, wz, ax, ay, az]\n",
    "        ang_vel = imu_data.angular_velocity\n",
    "        lin_acc = imu_data.linear_acceleration\n",
    "        return np.array([\n",
    "            ang_vel.x_val, ang_vel.y_val, ang_vel.z_val,\n",
    "            lin_acc.x_val, lin_acc.y_val, lin_acc.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_mag_data(self):\n",
    "        mag_data = self.client.getMagnetometerData()\n",
    "        # magnetometer.x, .y, .z\n",
    "        return np.array([\n",
    "            mag_data.magnetic_field_body.x_val,\n",
    "            mag_data.magnetic_field_body.y_val,\n",
    "            mag_data.magnetic_field_body.z_val\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_gps_data(self):\n",
    "        gps_data = self.client.getGpsData()\n",
    "        # gps_data.gnss.geo_point.latitude, .longitude, .altitude\n",
    "        return np.array([\n",
    "            gps_data.gnss.geo_point.latitude,\n",
    "            gps_data.gnss.geo_point.longitude,\n",
    "            gps_data.gnss.geo_point.altitude\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _get_baro_data(self):\n",
    "        baro_data = self.client.getBarometerData()\n",
    "        # baro_data.altitude, baro_data.pressure, baro_data.qnh\n",
    "        # We'll just use altitude\n",
    "        return np.array([baro_data.altitude], dtype=np.float32)\n",
    "\n",
    "    def _get_lidar_data(self):\n",
    "        \"\"\"\n",
    "        Retrieves LiDAR data from the 'LidarFront' sensor and returns a single float:\n",
    "        the minimum distance to an obstacle in front of the drone (or the max range if no hits).\n",
    "        \"\"\"\n",
    "        lidar_data = self.client.getLidarData(lidar_name=\"LidarFront\")\n",
    "        \n",
    "        # If no data or sensor not active\n",
    "        if not lidar_data.point_cloud or len(lidar_data.point_cloud) < 3:\n",
    "            # No points -> assume no obstacles within range\n",
    "            return np.array([1.0], dtype=np.float32)  # normalized '1.0' as 'no obstacle in range'\n",
    "        \n",
    "        points = np.array(lidar_data.point_cloud, dtype=np.float32).reshape(-1, 3)\n",
    "\n",
    "        distances = np.linalg.norm(points, axis=1)\n",
    "        min_distance = np.min(distances)\n",
    "\n",
    "        max_range = 4.0\n",
    "        clipped_dist = np.clip(min_distance, 0.0, max_range)\n",
    "        \n",
    "        # Normalize from 0..20 -> 0..1\n",
    "        #  (0 means extremely close obstacle, 1 means no obstacle within range)\n",
    "        normalized_dist = clipped_dist / max_range\n",
    "        return np.array([normalized_dist], dtype=np.float32)\n",
    "\n",
    "    def quaternion_to_euler(self, q):\n",
    "        \"\"\"\n",
    "        Convert AirSim Quaternionr (q.w_val, q.x_val, q.y_val, q.z_val)\n",
    "        to Euler angles (roll, pitch, yaw) in radians.\n",
    "        \n",
    "        AirSim uses a right-handed coordinate system:\n",
    "        - X axis: East\n",
    "        - Y axis: North\n",
    "        - Z axis: Down\n",
    "        \"\"\"\n",
    "        w = q.w_val\n",
    "        x = q.x_val\n",
    "        y = q.y_val\n",
    "        z = q.z_val\n",
    "\n",
    "        # Pre-calculate repeated values\n",
    "        ysqr = y * y\n",
    "\n",
    "        # Roll (x-axis rotation)\n",
    "        t0 = +2.0 * (w * x + y * z)\n",
    "        t1 = +1.0 - 2.0 * (x * x + ysqr)\n",
    "        roll = math.atan2(t0, t1)\n",
    "\n",
    "        # Pitch (y-axis rotation)\n",
    "        t2 = +2.0 * (w * y - z * x)\n",
    "        t2 = 1.0 if t2 > 1.0 else t2\n",
    "        t2 = -1.0 if t2 < -1.0 else t2\n",
    "        pitch = math.asin(t2)\n",
    "\n",
    "        # Yaw (z-axis rotation)\n",
    "        t3 = +2.0 * (w * z + x * y)\n",
    "        t4 = +1.0 - 2.0 * (ysqr + z * z)\n",
    "        yaw = math.atan2(t3, t4)\n",
    "\n",
    "        return roll, pitch, yaw\n",
    "\n",
    "    def to_quaternion(self, roll=0.0, pitch=0.0, yaw=0.0):\n",
    "        \"\"\"\n",
    "        Convert roll, pitch, yaw (in radians) to airsim.Quaternionr (x, y, z, w).\n",
    "        AirSim uses a right-handed coordinate system with X=East, Y=North, Z=Down.\n",
    "        \"\"\"\n",
    "        cy = math.cos(yaw * 0.5)\n",
    "        sy = math.sin(yaw * 0.5)\n",
    "        cr = math.cos(roll * 0.5)\n",
    "        sr = math.sin(roll * 0.5)\n",
    "        cp = math.cos(pitch * 0.5)\n",
    "        sp = math.sin(pitch * 0.5)\n",
    "\n",
    "        w = cy * cr * cp + sy * sr * sp\n",
    "        x = cy * sr * cp - sy * cr * sp\n",
    "        y = cy * cr * sp + sy * sr * cp\n",
    "        z = sy * cr * cp - cy * sr * sp\n",
    "\n",
    "        return airsim.Quaternionr(x_val=x, y_val=y, z_val=z, w_val=w)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        #Get and standardize observations.\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # We'll ignore GPS and magnetometer data as they're less relevant for this task\n",
    "        # and could add noise to the learning process\n",
    "\n",
    "        # Standardize position: Scale by area bounds\n",
    "        normalized_pos = position / np.array(self.current_params['area'])\n",
    "\n",
    "        # Standardize velocity: Most drones operate within -10 to 10 m/s range\n",
    "        normalized_vel = np.clip(velocity / 10.0, -1.0, 1.0)\n",
    "\n",
    "        # Standardize IMU data\n",
    "        angular_vel = imu_data[:3] / 10.0  # Angular velocity (rad/s)\n",
    "        linear_acc = imu_data[3:6] / 9.81  # Linear acceleration (normalize by g)\n",
    "\n",
    "        # Standardize target and distance\n",
    "        normalized_target = target / np.array(self.current_params['area'])\n",
    "        normalized_distance = distance / np.linalg.norm(self.current_params['area'])\n",
    "\n",
    "        # Height error (important for hovering)\n",
    "        height_error = (position[2] - target[2]) / self.current_params['area'][2]\n",
    "\n",
    "        # Combine all observations\n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]   - Normalized position (x, y, z)\n",
    "            normalized_vel,        # [3:6]   - Normalized velocity (vx, vy, vz)\n",
    "            angular_vel,          # [6:9]   - Normalized angular velocity (wx, wy, wz)\n",
    "            linear_acc,          # [9:12]  - Normalized linear acceleration (ax, ay, az)\n",
    "            normalized_target,    # [12:15] - Normalized target position\n",
    "            [normalized_distance],# [15]    - Normalized distance to target\n",
    "            [height_error]       # [16]    - Normalized height error\n",
    "        ])\n",
    "        # Clip all values to ensure they stay within a reasonable range\n",
    "        observation = np.clip(observation, -5.0, 5.0)\n",
    "        \n",
    "        return observation.astype(np.float32)\"\"\"\n",
    "        position = self._get_current_position()\n",
    "        velocity = self._get_current_velocity()\n",
    "        imu_data = self._get_imu_data()\n",
    "        target = self._get_target()\n",
    "        distance = self._get_distance_target()\n",
    "\n",
    "        # Normalize position relative to the current stage's boundaries\n",
    "        area_bounds = np.array(self.current_params['area'])\n",
    "        normalized_pos = np.clip(position / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize velocity (typical drone speeds rarely exceed ±10 m/s)\n",
    "        velocity_scale = np.array([10.0, 10.0, 5.0])  # Less vertical velocity range\n",
    "        normalized_vel = np.clip(velocity / velocity_scale, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize IMU data\n",
    "        # Angular velocity (typically ±5 rad/s)\n",
    "        angular_vel = np.clip(imu_data[:3] / 5.0, -1.0, 1.0)\n",
    "        # Linear acceleration (normalize by g ≈ 9.81 m/s²)\n",
    "        linear_acc = np.clip(imu_data[3:6] / 9.81, -3.0, 3.0)  # Allow for higher G-forces\n",
    "        \n",
    "        # Normalize target position\n",
    "        normalized_target = np.clip(target / area_bounds, -1.0, 1.0)\n",
    "        \n",
    "        # Normalize distance relative to maximum possible distance in current area\n",
    "        max_possible_distance = np.linalg.norm(area_bounds)\n",
    "        normalized_distance = np.clip(distance / max_possible_distance, 0.0, 1.0)\n",
    "        \n",
    "        # Height error normalization\n",
    "        max_height_error = area_bounds[2]\n",
    "        height_error = (position[2] - target[2]) / max_height_error\n",
    "        normalized_height_error = np.clip(height_error, -1.0, 1.0)\n",
    "\n",
    "        relative_pos = (target - position) / np.array(self.current_params['area'])\n",
    "        \n",
    "        state = self.client.getMultirotorState()\n",
    "        orientation = state.kinematics_estimated.orientation\n",
    "        roll, pitch, yaw = self.quaternion_to_euler(orientation)\n",
    "        yaw_sin = np.sin(yaw)\n",
    "        yaw_cos = np.cos(yaw)\n",
    "        \n",
    "        target_direction = (target - position) / (np.linalg.norm(target - position) + 1e-6)\n",
    "        relative_vel = np.dot(velocity, target_direction)\n",
    "        lidar_front = self._get_lidar_data()\n",
    "        observation = np.concatenate([\n",
    "            normalized_pos,        # [0:3]\n",
    "            normalized_vel,        # [3:6]\n",
    "            angular_vel,          # [6:9]\n",
    "            linear_acc,          # [9:12]\n",
    "            relative_pos,       # [12:15]\n",
    "            normalized_target,      # [15:18]\n",
    "            [normalized_distance],# [18]\n",
    "            [normalized_height_error],      # [19]\n",
    "            [relative_vel],       # [20] - Added velocity towards target\n",
    "            lidar_front,           # [21]\n",
    "            [yaw_sin, yaw_cos]      # [22:23]\n",
    "        ])\n",
    "        return observation.astype(np.float32)\n",
    "\n",
    "    def _apply_wind(self):\n",
    "        \"\"\"Applies a simulated wind force to the drone.\"\"\"\n",
    "        wind_x = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_y = np.random.uniform(-self.wind_factor, self.wind_factor)\n",
    "        wind_z = np.random.uniform(-self.wind_factor / 2, self.wind_factor / 2)\n",
    "        self.client.simSetWind(airsim.Vector3r(wind_x, wind_y, wind_z))\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        # Reset the drone\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(True)\n",
    "        self.client.armDisarm(True)\n",
    "        \n",
    "        params = self.current_params\n",
    "        spawn_x = random.uniform(-params['spawn_range'], params['spawn_range'])\n",
    "        spawn_y = random.uniform(-params['spawn_range'], params['spawn_range'])\n",
    "        spawn_z = random.uniform(-1,1)\n",
    "        spawn_yaw = random.uniform(-math.pi, math.pi)\n",
    "        \n",
    "        q = self.to_quaternion(0, 0, spawn_yaw)\n",
    "        pose = airsim.Pose(\n",
    "            airsim.Vector3r(spawn_x, spawn_y, spawn_z), q)\n",
    "\n",
    "        self.client.simSetVehiclePose(pose, True)\n",
    "        \n",
    "        self.client.takeoffAsync().join()\n",
    "        #self.client.moveToPositionAsync(spawn_x,spawn_y,spawn_z, 5).join()\n",
    "        #self.client.rotateToYawAsync(spawn_yaw).join()\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.consecutive_success = 0\n",
    "        self.episode += 1\n",
    "        self._apply_wind()\n",
    "        \n",
    "        self.episode_distances = []\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'current_target') or self.current_target is None:\n",
    "            self._get_target()  # This will generate a new target\n",
    "        \"\"\"\n",
    "        self._generate_new_target()\n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        if self.training_stage >= 7:\n",
    "                # Apply random action\n",
    "            if self.recovery_action_timer > 0:\n",
    "                self.recovery_action_timer -= 1\n",
    "            else:\n",
    "                if np.random.rand() <= 1/self.current_params['recovery_interval']:\n",
    "                    random_action = np.random.uniform(-1, 1, size=4)\n",
    "                    self.recovery_action_timer = 10  # Recovery period\n",
    "                    action = random_action\n",
    "        # Process controls - separate throttle handling\n",
    "        max_vx, max_vy, max_vz = 5.0, 5.0, 5.0   # [m/s]\n",
    "        max_yaw_deg = 45.0                      # [deg/s], or you can do rad/s\n",
    "\n",
    "        vx = float(action[0]) * max_vx\n",
    "        vy = float(action[1]) * max_vy\n",
    "        vz = float(action[2]) * max_vz\n",
    "        yaw_rate_deg = float(action[3]) * max_yaw_deg\n",
    "        \n",
    "        yaw_mode = airsim.YawMode(is_rate=True, yaw_or_rate=yaw_rate_deg)\n",
    "\n",
    "        # Move by velocity in BODY frame\n",
    "        self.client.moveByVelocityBodyFrameAsync(\n",
    "            vx=vx,\n",
    "            vy=vy,\n",
    "            vz=vz,\n",
    "            duration=0.25,  # small step\n",
    "            drivetrain=airsim.DrivetrainType.MaxDegreeOfFreedom,\n",
    "            yaw_mode=yaw_mode\n",
    "        ).join()\n",
    "        \n",
    "        observation = self._get_observation()\n",
    "        reward, terminated = self._calculate_reward()\n",
    "        \n",
    "        pos = self._get_current_position()\n",
    "        target = self._get_target()\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        self.episode_distances.append(distance)\n",
    "        self.episode_min_distance = min(self.episode_distances)\n",
    "        truncated = False\n",
    "        if self.current_step >= self.current_params['max_steps']:\n",
    "            truncated = True\n",
    "        #if truncated:\n",
    "        #    reward += 10\n",
    "        \"\"\"\n",
    "        if self.training_stage == 0:\n",
    "            # If we ended the episode by hitting max_steps (truncate) WITHOUT crashing (terminated=False),\n",
    "            # then we survived. Increment the counter.\n",
    "            if truncated:\n",
    "                self.hover_survival_count += 1\n",
    "                reward += 50\n",
    "\n",
    "                # If we've survived enough times in a row, move to stage 1\n",
    "                if self.hover_survival_count >= self.num_consecutive_survivals_needed:\n",
    "                    self.training_stage = 1\n",
    "                    self.setup_stage_params()\n",
    "                    self.hover_survival_count = 0\n",
    "                    print(\n",
    "                        f\"=== Moved to stage {self.training_stage} after \"\n",
    "                        f\"{self.num_consecutive_survivals_needed} consecutive survival episodes. ===\"\n",
    "                    )\n",
    "        \"\"\"\n",
    "        return observation, reward, terminated, truncated, {\"min_distance\": self.episode_min_distance}\n",
    "    def _calculate_reward(self):\n",
    "        pos = self._get_current_position()\n",
    "        vel = self._get_current_velocity()\n",
    "        angular_vel = self._get_imu_data()[:3]\n",
    "        target = self._get_target()\n",
    "        distance = np.linalg.norm(pos - target)\n",
    "        reward_scale = 0.005\n",
    "        reward = 0.0\n",
    "        # Immediate failure conditions (keep these)\n",
    "        if self.client.simGetCollisionInfo().has_collided:\n",
    "            reward -= 200*reward_scale\n",
    "            return reward, True\n",
    "        if any(abs(p) > a for p, a in zip(pos, self.current_params['area'])):\n",
    "            reward -= 100*reward_scale\n",
    "            return reward, True\n",
    "        # Main distance reward - smoother gradient\n",
    "        distance_reward = -distance  # Linear scaling for more consistent gradient\n",
    "        reward += distance_reward * reward_scale\n",
    "        \n",
    "        \"\"\"\n",
    "        # Stability penalty - much gentler\n",
    "        stability_penalty = (\n",
    "            0.05 * np.linalg.norm(angular_vel) +  # Reduced rotation penalty\n",
    "            0.02 * np.linalg.norm(vel)            # Reduced velocity penalty\n",
    "        )\n",
    "        reward -= stability_penalty\n",
    "        \n",
    "        if self.training_stage == 0:  # Hovering\n",
    "            # Pure hovering - focus on stability\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "            \n",
    "        elif self.training_stage == 1:  # Vertical movement\n",
    "            # Reward vertical progress toward target\n",
    "            vertical_distance = abs(pos[2] - target[2])\n",
    "            reward += (-vertical_distance + 1.0)*reward_scale\n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            # Penalize horizontal drift more strongly\n",
    "            horizontal_drift = np.linalg.norm(pos[:2] - target[:2])\n",
    "            reward -= (0.5 * horizontal_drift)*reward_scale\n",
    "            \n",
    "            reward += (distance_reward*0.5)*reward_scale\n",
    "            \n",
    "        elif self.training_stage == 2:  # Close-range horizontal\n",
    "            # Reward horizontal progress\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (3.0 * progress)*reward_scale  # Stronger reward for deliberate movement\n",
    "            self._prev_distance = distance\n",
    "            \n",
    "            # Height stability becomes secondary but still important\n",
    "            reward += (distance_reward)*reward_scale\n",
    "            \n",
    "        elif self.training_stage >= 3:  # Medium-range movement\n",
    "            # Balance distance and control\n",
    "            if hasattr(self, '_prev_distance'):\n",
    "                progress = self._prev_distance - distance\n",
    "                reward += (2.0 * progress)*reward_scale\n",
    "            self._prev_distance = distance\n",
    "            reward += (distance_reward)*reward_scale\n",
    "            \n",
    "        # Success condition\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance < success_radius:\n",
    "            reward += (10.0)*reward_scale\n",
    "            self.consecutive_success += 1\n",
    "            if self.consecutive_success >= self.success_threshold:\n",
    "                self.current_target = self._generate_new_target()\n",
    "                reward += 200.0*reward_scale  \n",
    "                self.success_count += 1\n",
    "                return reward, True\n",
    "            if self.success_count >= self.stage_threshold:\n",
    "                self.training_stage = min(8, self.training_stage + 1)\n",
    "                print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "                self.success_count = 0\n",
    "                self.setup_stage_params()\n",
    "                return reward, True\n",
    "        return reward, False\n",
    "        \"\"\"\n",
    "        if self.success_count >= self.stage_threshold:\n",
    "            self.training_stage = min(9, self.training_stage + 1)\n",
    "            print(f\"\\n=== Advanced to stage {self.training_stage} ===\")\n",
    "            self.success_count = 0\n",
    "            self.setup_stage_params()\n",
    "            return reward, True\n",
    "        success_radius = self.current_params['target_radius']\n",
    "        if distance <= success_radius:\n",
    "            reward += (2_000.0)*reward_scale\n",
    "            self.success_count += 1\n",
    "            return reward, True\n",
    "        return reward, False\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        self.client.reset()\n",
    "        self.client.enableApiControl(False)\n",
    "        self.client.armDisarm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print device info\n",
    "print(f\"Using device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# Create log directory\n",
    "log_dir = f\"./drone_training_logs/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Environment setup\n",
    "def make_env():\n",
    "    \"\"\"Helper function to create and wrap the AirSim environment\"\"\"\n",
    "    env = CosysAirSimEnv_Velocity(training_stage=0)  # Start with hover training\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = DummyVecEnv([make_env])\n",
    "eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0)\n",
    "\n",
    "# Create evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=f\"{log_dir}/best_model\",\n",
    "    log_path=f\"{log_dir}/eval_results\",\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=5,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(\n",
    "            pi=[256, 256],\n",
    "            vf=[256, 256]\n",
    "        ),\n",
    "        activation_fn=torch.nn.ReLU\n",
    "    ),\n",
    "    verbose=1,\n",
    "    tensorboard_log=f\"{log_dir}/tensorboard\"\n",
    ")\n",
    "\n",
    "# Custom callback for plotting\n",
    "class NotebookProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=1000, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.rewards = []\n",
    "        self.episodes = []\n",
    "        self.steps = []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        if len(self.training_env.buf_rews) > 0:\n",
    "            episode_reward = self.training_env.buf_rews[0]\n",
    "            self.rewards.append(episode_reward)\n",
    "            self.episodes.append(len(self.rewards))\n",
    "            self.steps.append(self.num_timesteps)\n",
    "            \n",
    "            if len(self.rewards) % self.check_freq == 0:\n",
    "                clear_output(wait=True)\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.plot(self.episodes, self.rewards)\n",
    "                plt.title('Training Progress')\n",
    "                plt.xlabel('Episode')\n",
    "                plt.ylabel('Reward')\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"Steps: {self.num_timesteps}\")\n",
    "                print(f\"Mean reward over last 100 episodes: {np.mean(self.rewards[-100:]):.2f}\")\n",
    "                print(f\"Current training stage: {self.training_env.envs[0].env.training_stage}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "# Training function\n",
    "def train_drone(total_timesteps=2_000_000):\n",
    "    progress_callback = NotebookProgressCallback()\n",
    "    try:\n",
    "        model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=[eval_callback, progress_callback],\n",
    "            progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Save final model\n",
    "        model.save(f\"{log_dir}/final_model\")\n",
    "        env.save(f\"{log_dir}/vec_normalize.pkl\")\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted! Saving current model...\")\n",
    "        model.save(f\"{log_dir}/interrupted_model\")\n",
    "        env.save(f\"{log_dir}/vec_normalize.pkl\")\n",
    "        print(\"Model saved!\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model_path, vec_normalize_path, n_eval_episodes=10):\n",
    "    \"\"\"Function to evaluate a trained model\"\"\"\n",
    "    # Load the saved statistics\n",
    "    eval_env = DummyVecEnv([make_env])\n",
    "    eval_env = VecNormalize.load(vec_normalize_path, eval_env)\n",
    "    eval_env.training = False\n",
    "    eval_env.norm_reward = False\n",
    "    \n",
    "    # Load the model\n",
    "    loaded_model = PPO.load(model_path)\n",
    "    \n",
    "    rewards = []\n",
    "    for episode in range(n_eval_episodes):\n",
    "        obs = eval_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = loaded_model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "    \n",
    "    print(f\"\\nAverage reward over {n_eval_episodes} episodes: {np.mean(rewards):.2f}\")\n",
    "    \n",
    "    # Plot evaluation results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_eval_episodes + 1), rewards, 'bo-')\n",
    "    plt.title('Evaluation Results')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_drone(total_timesteps=2_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "        # Use float32 for all arrays to match network precision\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(max_size, dtype=np.float32)  # Flattened array\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros(max_size, dtype=np.float32)    # Flattened array\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Ensure incoming data is on CPU and in numpy format\n",
    "        if torch.is_tensor(state):\n",
    "            state = state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(action):\n",
    "            action = action.detach().cpu().numpy()\n",
    "        if torch.is_tensor(reward):\n",
    "            reward = reward.detach().cpu().numpy()\n",
    "        if torch.is_tensor(next_state):\n",
    "            next_state = next_state.detach().cpu().numpy()\n",
    "        if torch.is_tensor(done):\n",
    "            done = done.detach().cpu().numpy()\n",
    "            \n",
    "        np.copyto(self.states[self.ptr], state)\n",
    "        np.copyto(self.actions[self.ptr], action)\n",
    "        self.rewards[self.ptr] = reward\n",
    "        np.copyto(self.next_states[self.ptr], next_state)\n",
    "        self.dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    \n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, obs_dim, act_dim, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize buffers with correct shapes\n",
    "        self.states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((max_size, act_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros((max_size, 1), dtype=np.float32)  # Changed shape to (max_size, 1)\n",
    "        self.next_states = np.zeros((max_size, obs_dim), dtype=np.float32)\n",
    "        self.dones = np.zeros((max_size, 1), dtype=np.float32)    # Changed shape to (max_size, 1)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to numpy arrays and ensure correct shapes\n",
    "        state = np.array(state, dtype=np.float32).flatten()\n",
    "        action = np.array(action, dtype=np.float32).flatten()\n",
    "        reward = np.array(reward, dtype=np.float32).reshape(1)\n",
    "        next_state = np.array(next_state, dtype=np.float32).flatten()\n",
    "        done = np.array(done, dtype=np.float32).reshape(1)\n",
    "\n",
    "        # Store transition\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        self.dones[self.ptr] = done\n",
    "\n",
    "        # Update pointer and size\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(self.states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.actions[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.rewards[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_states[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.dones[ind]).to(self.device)\n",
    "        )\n",
    "    def save(self, path):\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save the buffer state and metadata\n",
    "        save_dict = {\n",
    "            'max_size': self.max_size,\n",
    "            'ptr': self.ptr,\n",
    "            'size': self.size,\n",
    "            'states': self.states,\n",
    "            'actions': self.actions,\n",
    "            'rewards': self.rewards,\n",
    "            'next_states': self.next_states,\n",
    "            'dones': self.dones,\n",
    "            'device': self.device\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(save_dict, f)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to save buffer to {path}: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path, device=None):\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                save_dict = pickle.load(f)\n",
    "            \n",
    "            # Create new buffer with saved dimensions\n",
    "            obs_dim = save_dict['states'].shape[1]\n",
    "            act_dim = save_dict['actions'].shape[1]\n",
    "            buffer = ReplayBuffer(\n",
    "                max_size=save_dict['max_size'],\n",
    "                obs_dim=obs_dim,\n",
    "                act_dim=act_dim,\n",
    "                device=device or save_dict['device']\n",
    "            )\n",
    "            \n",
    "            # Restore buffer state\n",
    "            buffer.ptr = save_dict['ptr']\n",
    "            buffer.size = save_dict['size']\n",
    "            buffer.states = save_dict['states']\n",
    "            buffer.actions = save_dict['actions']\n",
    "            buffer.rewards = save_dict['rewards']\n",
    "            buffer.next_states = save_dict['next_states']\n",
    "            buffer.dones = save_dict['dones']\n",
    "            \n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load buffer from {path}: {e}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=1024): #512\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), #was LayerNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.BatchNorm1d(hidden_dim), #was LayerNorm\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        self._init_weights()\n",
    "        \"\"\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        \n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=1024): #512\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), #was LayerNorm\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(act_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), #was LayerNorm\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "        \"\"\"\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        q = self.net(sa)\n",
    "        return q\n",
    "    \"\"\"\n",
    "class TD3Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, max_action, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, act_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.__init_args__ = (obs_dim, act_dim, max_action, hidden_dim)\n",
    "        \n",
    "        # Use orthogonal initialization\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(layer.bias, 0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q1 architecture\n",
    "        self.q1_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Q2 architecture\n",
    "        self.q2_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.__init_args__ = (obs_dim, act_dim, hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for net in [self.q1_net, self.q2_net]:\n",
    "            for m in net.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        # Get Q-values\n",
    "        q1 = self.q1_net(sa)\n",
    "        q2 = self.q2_net(sa)\n",
    "        \n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        # Ensure inputs are properly shaped\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if action.dim() == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "            \n",
    "        # Concatenate state and action\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        return self.q1_net(sa)\n",
    "\"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.001):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self._hard_update_targets()\n",
    "        \n",
    "        self.target_entropy = -float(actor.__init_args__[1])\n",
    "        \n",
    "        # Initialize episode tracking\n",
    "        self.current_episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        \n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it):\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        reward = reward.view(-1, 1)\n",
    "        done = done.view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            noise = (torch.randn_like(action) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_Q1 = self.critic_target1(next_state, next_action)\n",
    "            target_Q2 = self.critic_target2(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "        \n",
    "        # Critic 1 update\n",
    "        current_Q1 = self.critic1(state, action)\n",
    "        critic_mse_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_l2_reg1 = 0 #0.00001 * sum(torch.sum(param ** 2) for param in self.critic1.parameters())\n",
    "        critic_loss1 = critic_mse_loss1 + critic_l2_reg1\n",
    "        \n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward() \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), 1.0)\n",
    "        self.critic_optimizer1.step()\n",
    "        \n",
    "        # Critic 2 update\n",
    "        current_Q2 = self.critic2(state, action)\n",
    "        critic_mse_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "        critic_l2_reg2 = 0 #0.00001 * sum(torch.sum(param ** 2) for param in self.critic2.parameters())\n",
    "        critic_loss2 = critic_mse_loss2 + critic_l2_reg2\n",
    "        \n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()  \n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), 1.0)\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        if total_it % policy_delay == 0:\n",
    "            actor_actions = self.actor(state)\n",
    "            actor_loss = -self.critic1(state, actor_actions).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "            self.actor_optimizer.step()\n",
    "            \n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None,\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'q_values1': target_Q1,\n",
    "            'q_values2': target_Q2,\n",
    "            'target_q_values': target_Q\n",
    "        }\n",
    "    \"\"\"\n",
    "class TD3Trainer:\n",
    "    def __init__(self, actor, critic1, critic2, actor_optimizer, critic_optimizer1, \n",
    "                 critic_optimizer2, max_action, device, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.actor_target = type(actor)(*actor.__init_args__).to(device)\n",
    "        self.critic_target1 = type(critic1)(*critic1.__init_args__).to(device)\n",
    "        self.critic_target2 = type(critic2)(*critic2.__init_args__).to(device)\n",
    "        \n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer1 = critic_optimizer1\n",
    "        self.critic_optimizer2 = critic_optimizer2\n",
    "        \n",
    "        self.max_action = max_action\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self._hard_update_targets()\n",
    "\n",
    "    def _hard_update_targets(self):\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target1.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic_target2.load_state_dict(self.critic2.state_dict())\n",
    "    \n",
    "    def _soft_update(self, target, source):\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.tau * param.data + (1.0 - self.tau) * target_param.data\n",
    "                )\n",
    "\n",
    "    def train_step(self, replay_buffer, batch_size, noise_std, noise_clip, policy_delay, total_it):\n",
    "        # Sample from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Ensure proper dimensions\n",
    "        reward = reward.view(-1, 1)\n",
    "        done = done.view(-1, 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (torch.randn_like(action) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target1(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, _ = self.critic1(state, action)\n",
    "        current_Q2, _ = self.critic2(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss1 = F.mse_loss(current_Q1, target_Q)\n",
    "        critic_loss2 = F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critics\n",
    "        self.critic_optimizer1.zero_grad()\n",
    "        critic_loss1.backward()\n",
    "        self.critic_optimizer1.step()\n",
    "\n",
    "        self.critic_optimizer2.zero_grad()\n",
    "        critic_loss2.backward()\n",
    "        self.critic_optimizer2.step()\n",
    "\n",
    "        actor_loss = None\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if total_it % policy_delay == 0:\n",
    "            # Compute actor loss\n",
    "            actor_action = self.actor(state)\n",
    "            Q1, _ = self.critic1(state, actor_action)\n",
    "            actor_loss = -Q1.mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            self._soft_update(self.actor_target, self.actor)\n",
    "            self._soft_update(self.critic_target1, self.critic1)\n",
    "            self._soft_update(self.critic_target2, self.critic2)\n",
    "\n",
    "        return {\n",
    "            'critic_loss1': critic_loss1.item(),\n",
    "            'critic_loss2': critic_loss2.item(),\n",
    "            'actor_loss': actor_loss.item() if actor_loss is not None else None\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def compute_parameter_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def evaluate_policy(actor, env, num_episodes=25, device='cuda'):\n",
    "    \"\"\"Evaluate the policy without exploration noise\"\"\"\n",
    "    eval_rewards = []\n",
    "    eval_success = 0\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                action = actor(state_tensor).cpu().numpy().flatten()\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if info.get('success', False):\n",
    "                eval_success += 1\n",
    "                break\n",
    "                \n",
    "        eval_rewards.append(episode_reward)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(eval_rewards),\n",
    "        'std_reward': np.std(eval_rewards),\n",
    "        'success_rate': eval_success / num_episodes,\n",
    "        'rewards': eval_rewards\n",
    "    }\n",
    "\n",
    "def run_training(env, obs_dim, act_dim, max_action, episodes=10000):\n",
    "    # Initialize environment and models\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize TensorBoard writer with more descriptive name\n",
    "    current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join('logs', f'TD3_train_{current_time}')\n",
    "    train_writer = SummaryWriter(log_dir + '/train')\n",
    "    eval_writer = SummaryWriter(log_dir + '/eval')\n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "    \n",
    "    # Model initialization\n",
    "    actor = TD3Actor(obs_dim, act_dim, max_action).to(device)\n",
    "    critic1 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    critic2 = TD3Critic(obs_dim, act_dim).to(device)\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=3e-6) #3e-4\n",
    "    critic_optimizer1 = optim.Adam(critic1.parameters(), lr=3e-6)\n",
    "    critic_optimizer2 = optim.Adam(critic2.parameters(), lr=3e-6)\n",
    "\n",
    "    # Load checkpoint if exists\n",
    "    total_steps = 0\n",
    "    current_stage = 0\n",
    "    best_rewards = {i: float('-inf') for i in range(9)}\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load('models/TD3/Model/mtp_backup_2250.pth')\n",
    "        actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "        critic1.load_state_dict(checkpoint['critic1_state_dict'])\n",
    "        critic2.load_state_dict(checkpoint['critic2_state_dict'])\n",
    "        actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        critic_optimizer1.load_state_dict(checkpoint['critic1_optimizer_state_dict'])\n",
    "        critic_optimizer2.load_state_dict(checkpoint['critic2_optimizer_state_dict'])\n",
    "        \n",
    "        try:\n",
    "            total_steps = checkpoint['total_steps']\n",
    "            current_stage = checkpoint['stage']\n",
    "            best_rewards = checkpoint['best_rewards']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        print(f\"Loaded checkpoint successfully. Total steps: {total_steps}\")\n",
    "    except:\n",
    "        print(\"No checkpoint found, starting fresh\")\n",
    "\n",
    "    # Initialize or load replay buffer\n",
    "    try:\n",
    "        replay_buffer = ReplayBuffer.load('models/TD3/Replay_Buffer/mtp_replay_buffer.pkl')\n",
    "        print(\"Loaded buffer successfully\")\n",
    "    except:\n",
    "        replay_buffer = ReplayBuffer(max_size=1_000_000, obs_dim=obs_dim, act_dim=act_dim)\n",
    "        print(\"No buffer found, starting fresh\")\n",
    "    \n",
    "    trainer = TD3Trainer(\n",
    "        actor=actor,\n",
    "        critic1=critic1,\n",
    "        critic2=critic2,\n",
    "        actor_optimizer=actor_optimizer,\n",
    "        critic_optimizer1=critic_optimizer1,\n",
    "        critic_optimizer2=critic_optimizer2,\n",
    "        max_action=max_action,\n",
    "        device=device\n",
    "    )\n",
    "    trainer._hard_update_targets()\n",
    "\n",
    "    # Training hyperparameters\n",
    "    batch_size = 512 #256\n",
    "    warmup_steps = 0 #20_000 #40_000\n",
    "    noise_std = 0.1\n",
    "    noise_clip = 0.75\n",
    "    policy_delay = 2\n",
    "    eval_freq = 200  # Evaluate every 200 episodes\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('models/TD3/Replay_Buffer', exist_ok=True)\n",
    "    os.makedirs('models/TD3/Model', exist_ok=True)\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    episode_rewards = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    stage_rewards = {i: [] for i in range(9)}\n",
    "    best_eval_reward = float('-inf')\n",
    "    \n",
    "    def get_exploration_noise(action):\n",
    "        action_tensor = torch.from_numpy(action)\n",
    "        base_noise = (torch.randn_like(action_tensor) * noise_std).clamp(-noise_clip, noise_clip)\n",
    "        return base_noise.cpu().numpy()\n",
    "    \n",
    "    def log_training_step(train_info, step):\n",
    "        \"\"\"\n",
    "        Enhanced logging function that tracks detailed metrics during training\n",
    "        \"\"\"\n",
    "        # Loss logging\n",
    "        train_writer.add_scalar('Loss/Critic1', train_info['critic_loss1'], step)\n",
    "        train_writer.add_scalar('Loss/Critic2', train_info['critic_loss2'], step)\n",
    "        if train_info['actor_loss'] is not None:\n",
    "            train_writer.add_scalar('Loss/Actor', train_info['actor_loss'], step)\n",
    "        \n",
    "        # Detailed gradient norm logging for each network\n",
    "        for name, param in actor.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                train_writer.add_scalar(f'Gradients/Actor/{name}_grad_norm', \n",
    "                                    param.grad.norm(2).item(), step)\n",
    "        \n",
    "        for name, param in critic1.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                train_writer.add_scalar(f'Gradients/Critic1/{name}_grad_norm', \n",
    "                                    param.grad.norm(2).item(), step)\n",
    "        \n",
    "        for name, param in critic2.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                train_writer.add_scalar(f'Gradients/Critic2/{name}_grad_norm', \n",
    "                                    param.grad.norm(2).item(), step)\n",
    "        \n",
    "        # Overall gradient norms\n",
    "        train_writer.add_scalar('Gradients/Actor_Total_Norm', compute_gradient_norm(actor), step)\n",
    "        train_writer.add_scalar('Gradients/Critic1_Total_Norm', compute_gradient_norm(critic1), step)\n",
    "        train_writer.add_scalar('Gradients/Critic2_Total_Norm', compute_gradient_norm(critic2), step)\n",
    "        \n",
    "        # Parameter value norms for each layer\n",
    "        for name, param in actor.named_parameters():\n",
    "            train_writer.add_scalar(f'Parameters/Actor/{name}_norm', \n",
    "                                param.norm(2).item(), step)\n",
    "        \n",
    "        for name, param in critic1.named_parameters():\n",
    "            train_writer.add_scalar(f'Parameters/Critic1/{name}_norm', \n",
    "                                param.norm(2).item(), step)\n",
    "        \n",
    "        for name, param in critic2.named_parameters():\n",
    "            train_writer.add_scalar(f'Parameters/Critic2/{name}_norm', \n",
    "                                param.norm(2).item(), step)\n",
    "        \n",
    "        # Overall parameter norms\n",
    "        train_writer.add_scalar('Parameters/Actor_Total_Norm', compute_parameter_norm(actor), step)\n",
    "        train_writer.add_scalar('Parameters/Critic1_Total_Norm', compute_parameter_norm(critic1), step)\n",
    "        train_writer.add_scalar('Parameters/Critic2_Total_Norm', compute_parameter_norm(critic2), step)\n",
    "        \n",
    "        # Learning rates\n",
    "        train_writer.add_scalar('LearningRate/Actor', actor_optimizer.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic1', critic_optimizer1.param_groups[0]['lr'], step)\n",
    "        train_writer.add_scalar('LearningRate/Critic2', critic_optimizer2.param_groups[0]['lr'], step)\n",
    "        \n",
    "        # Q-value statistics from critics\n",
    "        if 'q_values1' in train_info:\n",
    "            train_writer.add_scalar('Q_Values/Critic1_Mean', train_info['q_values1'].mean().item(), step)\n",
    "            train_writer.add_scalar('Q_Values/Critic1_Std', train_info['q_values1'].std().item(), step)\n",
    "            train_writer.add_histogram('Q_Values/Critic1_Distribution', train_info['q_values1'], step)\n",
    "        \n",
    "        if 'q_values2' in train_info:\n",
    "            train_writer.add_scalar('Q_Values/Critic2_Mean', train_info['q_values2'].mean().item(), step)\n",
    "            train_writer.add_scalar('Q_Values/Critic2_Std', train_info['q_values2'].std().item(), step)\n",
    "            train_writer.add_histogram('Q_Values/Critic2_Distribution', train_info['q_values2'], step)\n",
    "        \n",
    "        # Target Q-values if available\n",
    "        if 'target_q_values' in train_info:\n",
    "            train_writer.add_scalar('Q_Values/Target_Mean', train_info['target_q_values'].mean().item(), step)\n",
    "            train_writer.add_scalar('Q_Values/Target_Std', train_info['target_q_values'].std().item(), step)\n",
    "            train_writer.add_histogram('Q_Values/Target_Distribution', train_info['target_q_values'], step)\n",
    "            \n",
    "        if step % 100 == 0:\n",
    "            train_writer.flush()\n",
    "            eval_writer.flush()\n",
    "    \n",
    "    def run_evaluation(episode):\n",
    "        # Create a separate environment for evaluation\n",
    "        eval_env = env.__class__()  # Assuming env has a constructor that takes no arguments\n",
    "        eval_env.training_stage = env.training_stage  # Sync the training stage\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_results = evaluate_policy(actor, eval_env, num_episodes=10, device=device)\n",
    "        \n",
    "        # Log evaluation metrics\n",
    "        eval_writer.add_scalar('Eval/Mean_Reward', eval_results['mean_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Reward_Std', eval_results['std_reward'], episode)\n",
    "        eval_writer.add_scalar('Eval/Success_Rate', eval_results['success_rate'], episode)\n",
    "        \n",
    "        # Log reward distribution\n",
    "        eval_writer.add_histogram('Eval/Reward_Distribution', \n",
    "                                torch.tensor(eval_results['rewards']), \n",
    "                                episode)\n",
    "        \n",
    "        eval_env.close()\n",
    "        return eval_results\n",
    "    \n",
    "    progress_bar = tqdm.tqdm(range(episodes), desc=\"Training\")\n",
    "    for episode in progress_bar:\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        episode_steps = 0\n",
    "        current_stage = env.training_stage\n",
    "        episode_min_distance = float('inf')\n",
    "        # Log current stage\n",
    "        train_writer.add_scalar('Training/Current_Stage', current_stage, total_steps)\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            if total_steps < warmup_steps:\n",
    "                action = np.random.uniform(-max_action, max_action, size=act_dim)\n",
    "                train_writer.add_scalar('Training/Exploration_Type', 0, total_steps)  # 0 for random\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                    action = actor(state_tensor).cpu().numpy().flatten()\n",
    "                    current_noise = get_exploration_noise(action)\n",
    "                    action = action + current_noise\n",
    "                action = np.clip(action, -max_action, max_action)\n",
    "                train_writer.add_scalar('Training/Exploration_Type', 1, total_steps)  # 1 for policy\n",
    "                train_writer.add_scalar('Training/Exploration_Noise_Mean', np.mean(current_noise), total_steps)\n",
    "                train_writer.add_scalar('Training/Exploration_Noise_Std', np.std(current_noise), total_steps)\n",
    "                train_writer.add_scalar('Training/Exploration_Noise_Max', np.max(np.abs(current_noise)), total_steps)\n",
    "            \n",
    "            # Log action statistics\n",
    "            train_writer.add_histogram('Actions/Distribution', action, total_steps)\n",
    "            train_writer.add_scalar('Actions/Mean', np.mean(action), total_steps)\n",
    "            train_writer.add_scalar('Actions/Std', np.std(action), total_steps)\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if \"min_distance\" in info:\n",
    "                episode_min_distance = min(episode_min_distance, info[\"min_distance\"])\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            # Update state and metrics\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Train agent\n",
    "            if total_steps > warmup_steps and len(replay_buffer.states) > batch_size:\n",
    "                train_info = trainer.train_step(\n",
    "                    replay_buffer=replay_buffer,\n",
    "                    batch_size=batch_size,\n",
    "                    noise_std=noise_std,\n",
    "                    noise_clip=noise_clip,\n",
    "                    policy_delay=policy_delay,\n",
    "                    total_it=total_steps,\n",
    "                )\n",
    "                \n",
    "                # Log training metrics\n",
    "                log_training_step(train_info, total_steps)\n",
    "        \n",
    "        # Episode completion logging\n",
    "        recent_rewards.append(episode_reward)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        stage_rewards[current_stage].append(episode_reward)\n",
    "        \n",
    "        # Log episode metrics\n",
    "        train_writer.add_scalar('Episode/Reward', episode_reward, episode)\n",
    "        train_writer.add_scalar('Episode/Steps', episode_steps, episode)\n",
    "        train_writer.add_scalar('Episode/Average_100_Episodes', np.mean(recent_rewards), episode)\n",
    "        train_writer.add_scalar('Training/Success_Count', env.success_count, episode)\n",
    "        train_writer.add_scalar('Training/Buffer_Size', len(replay_buffer), episode)\n",
    "        train_writer.add_scalar('Episode/Min_Distance', episode_min_distance, episode)\n",
    "        # Run evaluation periodically\n",
    "        \"\"\"\n",
    "        if episode % eval_freq == 0 and episode >= 800:\n",
    "            eval_results = run_evaluation(episode)\n",
    "            \n",
    "            # Save best model based on evaluation\n",
    "            if eval_results['mean_reward'] > best_eval_reward:\n",
    "                best_eval_reward = eval_results['mean_reward']\n",
    "                torch.save({\n",
    "                    'actor_state_dict': actor.state_dict(),\n",
    "                    'critic1_state_dict': critic1.state_dict(),\n",
    "                    'critic2_state_dict': critic2.state_dict(),\n",
    "                    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                    'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                    'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                    'total_steps': total_steps,\n",
    "                    'episode': episode,\n",
    "                    'stage': current_stage,\n",
    "                    'eval_reward': best_eval_reward,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, 'models/TD3/Model/mtp_model_best_eval.pth')\n",
    "        \"\"\"\n",
    "        # Stage-specific metrics\n",
    "        if len(stage_rewards[current_stage]) >= 100:\n",
    "            current_avg_reward = np.mean(stage_rewards[current_stage][-100:])\n",
    "            train_writer.add_scalar(f'Stage_{current_stage}/Average_100_Episodes', \n",
    "                                  current_avg_reward, \n",
    "                                  len(stage_rewards[current_stage]))\n",
    "            \n",
    "            if current_avg_reward > best_rewards[current_stage]:\n",
    "                best_rewards[current_stage] = current_avg_reward\n",
    "                train_writer.add_scalar(f'Stage_{current_stage}/Best_Average_Reward', \n",
    "                                      current_avg_reward, \n",
    "                                      len(stage_rewards[current_stage]))\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'stage': current_stage,\n",
    "            'reward': f'{episode_reward:.2f}',\n",
    "            'avg_reward': f'{np.mean(recent_rewards):.2f}' if episode_rewards else 'N/A',\n",
    "            'success_count': env.success_count,\n",
    "            'min_dist': f'{episode_min_distance:.2f}' if episode_min_distance != float(\"inf\") else 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Check for training completion\n",
    "        if current_stage == 9 and env.success_count >= env.stage_threshold:\n",
    "            print(\"\\n=== Training completed successfully! ===\")\n",
    "            env.close()\n",
    "            print(\"Environment closed. Running final evaluation...\")\n",
    "            final_eval_results = run_evaluation(episode)\n",
    "            print(f\"Final evaluation results: {final_eval_results}\")\n",
    "            train_writer.close()\n",
    "            eval_writer.close()\n",
    "            \n",
    "            replay_buffer.save('models/TD3/Replay_Buffer/mtp_replay_buffer_final.pkl')\n",
    "            checkpoint_path = f'models/TD3/Model/mtp_model_final.pth'\n",
    "            torch.save({\n",
    "                    'actor_state_dict': actor.state_dict(),\n",
    "                    'critic1_state_dict': critic1.state_dict(),\n",
    "                    'critic2_state_dict': critic2.state_dict(),\n",
    "                    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                    'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                    'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                    'total_steps': total_steps,\n",
    "                    'episode': episode,\n",
    "                    'stage': current_stage,\n",
    "                    'eval_reward': best_eval_reward,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, checkpoint_path)\n",
    "            \n",
    "            break\n",
    "        \n",
    "        # Periodic checkpointing\n",
    "        if episode % 250 == 0:\n",
    "            replay_buffer.save('models/TD3/Replay_Buffer/mtp_replay_buffer.pkl')\n",
    "            train_writer.flush()\n",
    "            eval_writer.flush()\n",
    "            checkpoint_path = f'models/TD3/Model/mtp_backup_{episode}.pth'\n",
    "            torch.save({\n",
    "                    'actor_state_dict': actor.state_dict(),\n",
    "                    'critic1_state_dict': critic1.state_dict(),\n",
    "                    'critic2_state_dict': critic2.state_dict(),\n",
    "                    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
    "                    'critic1_optimizer_state_dict': critic_optimizer1.state_dict(),\n",
    "                    'critic2_optimizer_state_dict': critic_optimizer2.state_dict(),\n",
    "                    'total_steps': total_steps,\n",
    "                    'episode': episode,\n",
    "                    'stage': current_stage,\n",
    "                    'eval_reward': best_eval_reward,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "    return actor, critic1, critic2, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n",
      "Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)\n",
      "\n",
      "Using device: cuda\n",
      "TensorBoard logs will be saved to: logs\\TD3_train_20250127-195618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\henhe\\AppData\\Local\\Temp\\ipykernel_13140\\1026655346.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('models/TD3/Model/mtp_backup_2250.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint successfully. Total steps: 149848\n",
      "Loaded buffer successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/50000 [00:06<85:16:26,  6.14s/it, stage=5, reward=-1.88, avg_reward=-1.88, success_count=0, min_dist=8.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at models/TD3/Model/mtp_backup_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 5/50000 [03:58<663:34:11, 47.78s/it, stage=5, reward=-6.95, avg_reward=-4.52, success_count=0, min_dist=1.46] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m max_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh[\u001b[38;5;241m0\u001b[39m]) \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m actor, critic1, critic2, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50_000\u001b[39;49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 267\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m(env, obs_dim, act_dim, max_action, episodes)\u001b[0m\n\u001b[0;32m    264\u001b[0m train_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActions/Std\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mstd(action), total_steps)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info:\n",
      "Cell \u001b[1;32mIn[56], line 493\u001b[0m, in \u001b[0;36mCosysAirSimEnv_Velocity.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    483\u001b[0m yaw_mode \u001b[38;5;241m=\u001b[39m airsim\u001b[38;5;241m.\u001b[39mYawMode(is_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, yaw_or_rate\u001b[38;5;241m=\u001b[39myaw_rate_deg)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# Move by velocity in BODY frame\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoveByVelocityBodyFrameAsync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# small step\u001b[39;49;00m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrivetrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mairsim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDrivetrainType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxDegreeOfFreedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43myaw_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myaw_mode\u001b[49m\n\u001b[1;32m--> 493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_observation()\n\u001b[0;32m    496\u001b[0m reward, terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward()\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\msgpackrpc\\future.py:22\u001b[0m, in \u001b[0;36mFuture.join\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_flag):\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\msgpackrpc\\loop.py:22\u001b[0m, in \u001b[0;36mLoop.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Starts the Tornado's ioloop if it's not running.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ioloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\msgpackrpc\\tornado\\ioloop.py:863\u001b[0m, in \u001b[0;36mPollIOLoop.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m     signal\u001b[38;5;241m.\u001b[39msetitimer(signal\u001b[38;5;241m.\u001b[39mITIMER_REAL, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 863\u001b[0m     event_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;66;03m# Depending on python version and IOLoop implementation,\u001b[39;00m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;66;03m# different exception types may be thrown and there are\u001b[39;00m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;66;03m# two ways EINTR might be signaled:\u001b[39;00m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;66;03m# * e.errno == errno.EINTR\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# * e.args is like (errno.EINTR, 'Interrupted system call')\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_from_exception(e) \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINTR:\n",
      "File \u001b[1;32mc:\\Users\\henhe\\anaconda3\\envs\\py3\\Lib\\site-packages\\msgpackrpc\\tornado\\platform\\select.py:62\u001b[0m, in \u001b[0;36m_Select.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpoll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[1;32m---> 62\u001b[0m     readable, writeable, errors \u001b[38;5;241m=\u001b[39m \u001b[43mselect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     events \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fd \u001b[38;5;129;01min\u001b[39;00m readable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = CosysAirSimEnv_Velocity()\n",
    "\n",
    "# Get environment dimensions\n",
    "obs_dim = env.observation_space.shape[0]  \n",
    "act_dim = env.action_space.shape[0]     \n",
    "max_action = float(env.action_space.high[0]) \n",
    "\n",
    "# Run training\n",
    "actor, critic1, critic2, rewards = run_training(\n",
    "    env=env,\n",
    "    obs_dim=obs_dim,\n",
    "    act_dim=act_dim,\n",
    "    max_action=max_action,\n",
    "    episodes= 50_000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
